{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/D2I-Melbourne/MOP/images/mop-black-100px.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/D2I-Melbourne/MOP/blob/master/images/mop-black-100px.png)\n",
    "# MELBOURNE CITY OPEN DATA PLAYGROUND\n",
    "---\n",
    "## CLUE Cafe, restaurant, bistro seats\n",
    "## Exploratory Data Analysis\n",
    "---\n",
    "| Date | Author/Contributor | Change |\n",
    "| :- | :- | :- |\n",
    "| 18-Nov-2021 | Steven Tuften | Initial Draft |\n",
    "| 26-Nov-2021 | Steven Tuften | Added d2i_tools.py in parent folder |\n",
    "| 28-Nov-2021 | Steven Tuften | Replaced d2i_tools.py with d2i_tools2.py in parent folder |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6W1OOykftSZ",
    "outputId": "4f878d2d-7100-4fff-a841-38ed30600566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis==3.2.2\n",
      "  Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pyLDAvis==3.2.2) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pyLDAvis==3.2.2) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pyLDAvis==3.2.2) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pyLDAvis==3.2.2) (0.17.0)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pyLDAvis==3.2.2) (2.11.2)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.0-cp38-cp38-win_amd64.whl (88 kB)\n",
      "Processing c:\\users\\61412\\appdata\\local\\pip\\cache\\wheels\\8e\\70\\28\\3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\\future-0.18.2-py3-none-any.whl\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pyLDAvis==3.2.2) (1.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis==3.2.2) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==3.2.2) (1.15.0)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (setup.py): started\n",
      "  Building wheel for pyLDAvis (setup.py): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135598 sha256=b2c47dd84f453cb6ccc7f3e0fd3c8f75e5bf85d7b25a7eed819caedc05deb056\n",
      "  Stored in directory: c:\\users\\61412\\appdata\\local\\pip\\cache\\wheels\\2a\\5b\\b3\\26b52781cdeea9c815e147cfd4ac4a0a3472bce92142115670\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: numexpr, future, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.16 future-0.18.2 numexpr-2.8.0 pyLDAvis-3.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis==3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2021.11.10-cp38-cp38-win_amd64.whl (273 kB)\n",
      "Requirement already satisfied: click in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Installing collected packages: regex, tqdm, nltk\n",
      "Successfully installed nltk-3.6.5 regex-2021.11.10 tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.1-cp38-cp38-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from spacy) (1.19.1)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from spacy) (50.3.0.post20201006)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from spacy) (20.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: catalogue, srsly, typer, spacy-legacy, wasabi, spacy-loggers, typing-extensions, pydantic, murmurhash, blis, cymem, preshed, thinc, langcodes, smart-open, pathy, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 typing-extensions-4.0.1 wasabi-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from gensim) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\61412\\anaconda3\\envs\\melbournecityopendata\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Installing collected packages: Cython, gensim\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTOHtH3r8VE-",
    "outputId": "feefe983-2e03-4a07-9494-7400519eb822"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\61412\\anaconda3\\envs\\MelbourneCityOpenData\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\61412\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from sodapy import Socrata\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting libraries\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt# NLTK Stop words\n",
    "import seaborn as sns \n",
    "\n",
    "# NLP Libraries\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AS1dKGRq75sJ"
   },
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "# get api token\n",
    "client = Socrata(\"data.melbourne.vic.gov.au\", #domain\n",
    "                'Fake',  #app token\n",
    "                username=\"Fake\", #api id\n",
    "                password=\"Fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QqPXhi28xpI"
   },
   "source": [
    "## First step obtain datasets: \n",
    "  - dataset name \n",
    "  - id \n",
    "  - metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "elyHKgNhLR-R",
    "outputId": "bc9caf45-d082-4ecf-e709-4f6553a2705a"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-847499b2de10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   content.append([a['resource']['name'],\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36mdatasets\u001b[0;34m(self, limit, offset, order, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         results = self._perform_request(\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASETS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"offset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m         \u001b[0mnumResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resultSetSize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request_type, resource, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# handle errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# when responses have no content body (ie. delete, set_permission),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sodapy/utils.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmore_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmore_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mhttp_error_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\".\\n\\t{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "for a in client.datasets():\n",
    "\n",
    "  content.append([a['resource']['name'],\n",
    "\n",
    "  a['resource']['columns_name'],\n",
    "  a['resource']['description'],\n",
    "  a['classification']['categories'],\n",
    "  a['classification']['domain_category'],\n",
    "  a['classification']['tags'],\n",
    "  a['classification']['domain_tags']])\n",
    "\n",
    "\n",
    "\n",
    "# remove '[]' and join string\n",
    "content_lst_inital = []\n",
    "for each_element in content:\n",
    "  each_element_list = []\n",
    "  for element in each_element:\n",
    "    if type(element) == list:\n",
    "      element = ' '.join(element)\n",
    "    else:\n",
    "      pass\n",
    "    each_element_list.append(element)\n",
    "  cont= (' ').join(each_element_list)\n",
    "  content_lst_inital.append(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpFbUYYzLWkP"
   },
   "outputs": [],
   "source": [
    "content = []\n",
    "for a in client.datasets():\n",
    "\n",
    "  content.append([a['resource']['name'],\n",
    "  a['resource']['columns_name'],\n",
    "  a['resource']['description'],\n",
    "  a['classification']['categories'],\n",
    "  a['classification']['domain_category'],\n",
    "  a['classification']['tags'],\n",
    "  a['classification']['domain_tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbP5R9OHLHC-"
   },
   "outputs": [],
   "source": [
    "total_list = []\n",
    "for each_sent in content:\n",
    "  test = []\n",
    "  for i in each_sent:\n",
    "    if type(i) == list:\n",
    "      i = \" \".join(i)\n",
    "      test.append(i)\n",
    "    else:\n",
    "      test.append(i)\n",
    "  line_list = \" \".join(test)\n",
    "  total_list.append(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04jv0Vj9Z5yQ"
   },
   "outputs": [],
   "source": [
    "re.sub(r'[\\(\\)]',\"\", '(counts per ) (. ) ( ) (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC0Gnbx-XLFJ"
   },
   "outputs": [],
   "source": [
    "# remove emails\n",
    "data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in total_list]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "# html tags\n",
    "data = [re.sub(r\"<.*?> \", \"\", sent) for sent in data]\n",
    "\n",
    "#\n",
    "data = [re.sub(r'[^a-zA-Z]+', \" \", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAr5yGnoXvdZ"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEHEgmBvbivB"
   },
   "outputs": [],
   "source": [
    "!pip install importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4urvb04enl6"
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI3z-Yj0ewqL"
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IybIg4ybAOr"
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avQIk4xjVVyx"
   },
   "outputs": [],
   "source": [
    "data_lemmatized[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Xn37Jyydr8I"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0IxCWNddvqt"
   },
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_OMYFVCeOVr"
   },
   "source": [
    "## Meta Data cleaning Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7norQiFNWW1J"
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=14, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSSF1ofJfU4c"
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoG6LZohV8jC"
   },
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrWPj6SOVoAq"
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC6hA8g4fgVq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVAMGiXsWaDk"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56cAMT2uOi20"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# Install spaCy (run in terminal/prompt)\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Download spaCy's  'en' Model\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kVGnFTCXHLi"
   },
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv6tUle-Nug4"
   },
   "outputs": [],
   "source": [
    "each_char = 'trees'\n",
    "\n",
    "lemmatizer.lemmatize(each_char, get_wordnet_pos(each_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEJ0Be34Kmmz"
   },
   "outputs": [],
   "source": [
    "nlp_content = []\n",
    "for each_sentence in words:\n",
    "  list_1 = []\n",
    "  for each_char in nltk.word_tokenize(each_sentence):\n",
    "    tk=lemmatizer.lemmatize(each_char, get_wordnet_pos(each_char))\n",
    "    list_1.append(tk)\n",
    "  nlp_content.append(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSah2iTEhU1t"
   },
   "outputs": [],
   "source": [
    "nlp_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpDsNWcTRpCo"
   },
   "outputs": [],
   "source": [
    "## check sparsicity \n",
    "\n",
    "# make it to sentence \n",
    "# nlp_content_string= sentence \n",
    "# nlp_content = nlp_content_string.split(' ')\n",
    "nlp_content_string = [' '.join(i) for i in nlp_content]\n",
    "\n",
    "\n",
    "#remove built-in english stopwords, convert all words to lowercase, \n",
    "#and a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLVpBTAHRuma"
   },
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer.fit_transform(nlp_content_string)\n",
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvLEUY_ut693"
   },
   "outputs": [],
   "source": [
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knpl8cA6SA8j"
   },
   "outputs": [],
   "source": [
    "search_params = {'n_components': list(range(1,223)), 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-06sx2Ehiuyg"
   },
   "outputs": [],
   "source": [
    "test_model_14 =LatentDirichletAllocation(batch_size=6, n_components = 14,\n",
    "                        random_state=0,\n",
    "                        learning_decay=0.7,\n",
    "                        max_iter= 10,\n",
    "                        max_doc_update_iter=100,\n",
    "                        learning_method='batch')\n",
    "test_model_14.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "726ItfqAILKE"
   },
   "outputs": [],
   "source": [
    "test_model_6 =LatentDirichletAllocation(batch_size=6, n_components = 6,\n",
    "                        random_state=0,\n",
    "                        learning_decay=0.7,\n",
    "                        max_iter= 10,\n",
    "                        max_doc_update_iter=100,\n",
    "                        learning_method='batch')\n",
    "test_model_6.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jtvO2-K9jTV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data is data_vectorized\n",
    "def return_topic_visual_pro_topic(model, data=data_vectorized):\n",
    "\n",
    "  # Create Document - Topic Matrix\n",
    "  lda_output = model.transform(data)\n",
    "  # column names\n",
    "  topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "  # index names\n",
    "  docnames = [\"dataset\" + str(i) for i in range(data.shape[0])]\n",
    "\n",
    "  # Make the pandas dataframe\n",
    "  df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "  # Get dominant topic for each document\n",
    "  dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "  df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "  # Styling\n",
    "  def color_green(val):\n",
    "      color = 'green' if val > .1 else 'black'\n",
    "      return 'color: {col}'.format(col=color)\n",
    "\n",
    "  def make_bold(val):\n",
    "      weight = 700 if val > .1 else 400\n",
    "      return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "\n",
    "  # Apply Style\n",
    "  df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "  \n",
    "\n",
    "  return  df_document_topics, dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXUCA8buHfb3"
   },
   "outputs": [],
   "source": [
    "# replace model to see difference \n",
    "df_document_topics, dominant_topic  = return_topic_visual_pro_topic(test_model_14, data_vectorized)\n",
    "df_document_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wSM3QDIuwE"
   },
   "outputs": [],
   "source": [
    "#topic_n = df_document_topic['dominant_topic']\n",
    "#df.loc[:,'topic_n'] = topic_n.values\n",
    "df['content'] = nlp_content_string\n",
    "df['topic_n'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YM2aaxvjCzaj"
   },
   "outputs": [],
   "source": [
    "df[df.loc[:,'topic_n'] == 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diL3GPBZD30Q"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(test_model_14, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQS3iErJJAHb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

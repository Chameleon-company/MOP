{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "TP_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install pyLDAvis==3.2.2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis==3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (0.37.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.16)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.21.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.0.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (2.7.3)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (1.3.3)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.2) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==3.2.2) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.2) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==3.2.2) (1.15.0)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6W1OOykftSZ",
        "outputId": "4f878d2d-7100-4fff-a841-38ed30600566"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from sodapy import Socrata\n",
        "import seaborn as sns \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import matplotlib.pyplot as plt# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTOHtH3r8VE-",
        "outputId": "feefe983-2e03-4a07-9494-7400519eb822"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from sodapy import Socrata\n",
        "# get api token\n",
        "client = Socrata(\"data.melbourne.vic.gov.au\", #domain\n",
        "                'Fake',  #app token\n",
        "                username=\"Fake\", #api id\n",
        "                password=\"Fake\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "AS1dKGRq75sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First step obtain datasets: \n",
        "  - dataset name \n",
        "  - id \n",
        "  - metadata"
      ],
      "metadata": {
        "id": "1QqPXhi28xpI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "content = []\n",
        "for a in client.datasets():\n",
        "\n",
        "  content.append([a['resource']['name'],\n",
        "\n",
        "  a['resource']['columns_name'],\n",
        "  a['resource']['description'],\n",
        "  a['classification']['categories'],\n",
        "  a['classification']['domain_category'],\n",
        "  a['classification']['tags'],\n",
        "  a['classification']['domain_tags']])\n",
        "\n",
        "\n",
        "\n",
        "# remove '[]' and join string\n",
        "content_lst_inital = []\n",
        "for each_element in content:\n",
        "  each_element_list = []\n",
        "  for element in each_element:\n",
        "    if type(element) == list:\n",
        "      element = ' '.join(element)\n",
        "    else:\n",
        "      pass\n",
        "    each_element_list.append(element)\n",
        "  cont= (' ').join(each_element_list)\n",
        "  content_lst_inital.append(cont)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-847499b2de10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   content.append([a['resource']['name'],\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36mdatasets\u001b[0;34m(self, limit, offset, order, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         results = self._perform_request(\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASETS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"offset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m         \u001b[0mnumResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resultSetSize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request_type, resource, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# handle errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# when responses have no content body (ie. delete, set_permission),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sodapy/utils.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmore_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmore_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mhttp_error_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\".\\n\\t{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable"
          ]
        }
      ],
      "metadata": {
        "id": "elyHKgNhLR-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "bc9caf45-d082-4ecf-e709-4f6553a2705a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "content = []\n",
        "for a in client.datasets():\n",
        "\n",
        "  content.append([a['resource']['name'],\n",
        "  a['resource']['columns_name'],\n",
        "  a['resource']['description'],\n",
        "  a['classification']['categories'],\n",
        "  a['classification']['domain_category'],\n",
        "  a['classification']['tags'],\n",
        "  a['classification']['domain_tags']])"
      ],
      "outputs": [],
      "metadata": {
        "id": "OpFbUYYzLWkP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "total_list = []\n",
        "for each_sent in content:\n",
        "  test = []\n",
        "  for i in each_sent:\n",
        "    if type(i) == list:\n",
        "      i = \" \".join(i)\n",
        "      test.append(i)\n",
        "    else:\n",
        "      test.append(i)\n",
        "  line_list = \" \".join(test)\n",
        "  total_list.append(line_list)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BbP5R9OHLHC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "re.sub(r'[\\(\\)]',\"\", '(counts per ) (. ) ( ) (')"
      ],
      "outputs": [],
      "metadata": {
        "id": "04jv0Vj9Z5yQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# remove emails\n",
        "data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in total_list]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "# html tags\n",
        "data = [re.sub(r\"<.*?> \", \"\", sent) for sent in data]\n",
        "\n",
        "#\n",
        "data = [re.sub(r'[^a-zA-Z]+', \" \", sent) for sent in data]"
      ],
      "outputs": [],
      "metadata": {
        "id": "LC0Gnbx-XLFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "uAr5yGnoXvdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install importlib_metadata"
      ],
      "outputs": [],
      "metadata": {
        "id": "SEHEgmBvbivB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ],
      "outputs": [],
      "metadata": {
        "id": "H4urvb04enl6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "outputs": [],
      "metadata": {
        "id": "kI3z-Yj0ewqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "2IybIg4ybAOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_lemmatized[0:8]"
      ],
      "outputs": [],
      "metadata": {
        "id": "avQIk4xjVVyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "_Xn37Jyydr8I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "outputs": [],
      "metadata": {
        "id": "k0IxCWNddvqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta Data cleaning Done"
      ],
      "metadata": {
        "id": "j_OMYFVCeOVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=14, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7norQiFNWW1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "outputs": [],
      "metadata": {
        "id": "dSSF1ofJfU4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table"
      ],
      "metadata": {
        "id": "MoG6LZohV8jC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "outputs": [],
      "metadata": {
        "id": "SrWPj6SOVoAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "WC6hA8g4fgVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head(5)"
      ],
      "outputs": [],
      "metadata": {
        "id": "TVAMGiXsWaDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# Install spaCy (run in terminal/prompt)\n",
        "import sys\n",
        "!{sys.executable} -m pip install spacy\n",
        "\n",
        "# Download spaCy's  'en' Model\n",
        "!{sys.executable} -m spacy download en"
      ],
      "outputs": [],
      "metadata": {
        "id": "56cAMT2uOi20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "-kVGnFTCXHLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "each_char = 'trees'\n",
        "\n",
        "lemmatizer.lemmatize(each_char, get_wordnet_pos(each_char))"
      ],
      "outputs": [],
      "metadata": {
        "id": "gv6tUle-Nug4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nlp_content = []\n",
        "for each_sentence in words:\n",
        "  list_1 = []\n",
        "  for each_char in nltk.word_tokenize(each_sentence):\n",
        "    tk=lemmatizer.lemmatize(each_char, get_wordnet_pos(each_char))\n",
        "    list_1.append(tk)\n",
        "  nlp_content.append(list_1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "GEJ0Be34Kmmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nlp_content[0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "xSah2iTEhU1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## check sparsicity \n",
        "\n",
        "# make it to sentence \n",
        "# nlp_content_string= sentence \n",
        "# nlp_content = nlp_content_string.split(' ')\n",
        "nlp_content_string = [' '.join(i) for i in nlp_content]\n",
        "\n",
        "\n",
        "#remove built-in english stopwords, convert all words to lowercase, \n",
        "#and a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.\n",
        "vectorizer = CountVectorizer(analyzer='word',       \n",
        "                             min_df=10,                        # minimum reqd occurences of a word \n",
        "                             stop_words='english',             # remove stop words\n",
        "                             lowercase=True,                   # convert all words to lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                             # max_features=50000,             # max number of uniq words\n",
        "                            )\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tpDsNWcTRpCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_vectorized = vectorizer.fit_transform(nlp_content_string)\n",
        "# Materialize the sparse data\n",
        "data_dense = data_vectorized.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "MLVpBTAHRuma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_vectorized.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "kvLEUY_ut693"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "search_params = {'n_components': list(range(1,223)), 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Knpl8cA6SA8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_model_14 =LatentDirichletAllocation(batch_size=6, n_components = 14,\n",
        "                        random_state=0,\n",
        "                        learning_decay=0.7,\n",
        "                        max_iter= 10,\n",
        "                        max_doc_update_iter=100,\n",
        "                        learning_method='batch')\n",
        "test_model_14.fit(data_vectorized)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-06sx2Ehiuyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_model_6 =LatentDirichletAllocation(batch_size=6, n_components = 6,\n",
        "                        random_state=0,\n",
        "                        learning_decay=0.7,\n",
        "                        max_iter= 10,\n",
        "                        max_doc_update_iter=100,\n",
        "                        learning_method='batch')\n",
        "test_model_6.fit(data_vectorized)"
      ],
      "outputs": [],
      "metadata": {
        "id": "726ItfqAILKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "#data is data_vectorized\n",
        "def return_topic_visual_pro_topic(model, data=data_vectorized):\n",
        "\n",
        "  # Create Document - Topic Matrix\n",
        "  lda_output = model.transform(data)\n",
        "  # column names\n",
        "  topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
        "  # index names\n",
        "  docnames = [\"dataset\" + str(i) for i in range(data.shape[0])]\n",
        "\n",
        "  # Make the pandas dataframe\n",
        "  df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n",
        "  # Get dominant topic for each document\n",
        "  dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "  df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "  # Styling\n",
        "  def color_green(val):\n",
        "      color = 'green' if val > .1 else 'black'\n",
        "      return 'color: {col}'.format(col=color)\n",
        "\n",
        "  def make_bold(val):\n",
        "      weight = 700 if val > .1 else 400\n",
        "      return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n",
        "\n",
        "  # Apply Style\n",
        "  df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
        "  \n",
        "\n",
        "  return  df_document_topics, dominant_topic\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "3jtvO2-K9jTV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# replace model to see difference \n",
        "df_document_topics, dominant_topic  = return_topic_visual_pro_topic(test_model_14, data_vectorized)\n",
        "df_document_topics\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "BXUCA8buHfb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#topic_n = df_document_topic['dominant_topic']\n",
        "#df.loc[:,'topic_n'] = topic_n.values\n",
        "df['content'] = nlp_content_string\n",
        "df['topic_n'] = dominant_topic"
      ],
      "outputs": [],
      "metadata": {
        "id": "96wSM3QDIuwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df[df.loc[:,'topic_n'] == 13]"
      ],
      "outputs": [],
      "metadata": {
        "id": "YM2aaxvjCzaj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(test_model_14, data_vectorized, vectorizer, mds='tsne')\n",
        "panel"
      ],
      "outputs": [],
      "metadata": {
        "id": "diL3GPBZD30Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "SQS3iErJJAHb"
      }
    }
  ]
}
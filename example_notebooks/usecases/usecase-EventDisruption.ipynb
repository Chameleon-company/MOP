{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:2.0em;color:white; background-color: black\">&emsp;<b>Event Impacts and Effects on Pedestrian Traffic</b></p>\n",
    "    \n",
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:black; background-color: #DDDDDD; text-align:justify\">&emsp;<b>Authored by: </b>Alex Voung, Mark Brooksby, Brendan Richards</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black; text-align:right\"><b>Duration:</b> 30 to 120 mins&emsp;</p>\n",
    "\n",
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:black; background-color: #DDDDDD; text-align:justify\">&emsp;<b>Level: </b>Intermediate to Advanced&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<b>Pre-requisite Skills:</b>Python</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\">&emsp;<b>Scenario</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"My business relies on customer walk-ins. I want to understand the events or factors that influence pedestrian numbers near my business.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\">&emsp;<b>What this Use Case will teach you</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this use case you will understand how to:\n",
    "- Access and do exploratory analysis on the City of Melbourne's pedestrian sensor data.\n",
    "- Gain insights through integrating other relevant datasets including:\n",
    "    - Accessing more open data available through the City of Melbourne.\n",
    "    - Finding other open data on the internet and importing it for analysis.\n",
    "    - Connecting to real-time public transport data.\n",
    "- Create engaging visualisations of your analysis.\n",
    "- Create intuitive, interactive interfaces.\n",
    "- Perform predictive modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\">&emsp;<b>Enriching and Analysing the City of Melbourne's Pedestrian Sensor Network Data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The City of Melbourne produces an attractive interface that shows the location of pedestrian sensors around the CBD overlaid on a 3-Dimensional map. The interface can be animated at the click of a button, showing how the pedestrian traffic ebbs and flows as each hour passes. The interface also provides a comparison to the numbers of pedestrians moving past that sensor 4 weeks or 52 weeks prior.\n",
    "\n",
    "However, this interface doesn't explore the question of 'why' those numbers change. So this use case will attempt to do just that, by showcasing different approaches to investigating and analysing factors that can influence pedestrian traffic.\n",
    "\n",
    "Approach 1 will begin by comparing pedestrian traffic data with events in or near the CBD, such as AFL matches or the Melbourne Grand Prix. It will focus on statistical analysis and visualisation to show what impacts these events have.\n",
    "\n",
    "Approach 2 will show how to access and integrate real-time data from the VicRoads open data portal. It will present a visualisation of train positions, movement and capacity - providing an intuitive way to understand the impact of public transport on pedestrain traffic.\n",
    "\n",
    "Approach 3 will integrate data from the City of Melbourne's microclimate sensor network, as well as other promising datasets, to build regression models of pedestrian traffic. It will show how to create an interactive visualisation to allow users to experiment with the model.\n",
    "\n",
    "Each approach introduces new datasets, new ideas and will require the installation of new modules. They are presented in a way where each approach builds on the one prior to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\"><b>&emsp;What data, packages and accesses will I need?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The star of the show is - naturally - the open data provided by the City of Melbourne.\n",
    "In this notebook we will be using three of their datasets.\n",
    "\n",
    "\n",
    "<a href='https://data.melbourne.vic.gov.au/Transport/Pedestrian-Counting-System-Monthly-counts-per-hour/b2ak-trbp'>Pedestrian Counting System - Monthly (counts per hour)</a><br>\n",
    "<a href='https://data.melbourne.vic.gov.au/Transport/Pedestrian-Counting-System-Sensor-Locations/h57g-5234'>Pedestrian Counting System - Sensor Locations</a><br>\n",
    "<a href='https://data.melbourne.vic.gov.au/Environment/Microclimate-Sensor-Readings/u4vh-84j8'>Microclimate Sensor Readings</a><br>\n",
    "\n",
    "We also access the open data provided by VicRoads.\n",
    "\n",
    "<a href='https://data-exchange.vicroads.vic.gov.au/docs/services/vehicle-position-trip-update-opendata/operations/metro-train-service-alerts?'>Metro Train Service Alerts</a><br>\n",
    "<a href='https://data-exchange.vicroads.vic.gov.au/docs/services/vehicle-position-trip-update-opendata/operations/get-metrotraintripupdates?'>Metro Trains Trip Updates</a><br>\n",
    "<a href='https://data-exchange.vicroads.vic.gov.au/docs/services/vehicle-position-trip-update-opendata/operations/get-metrotrainvehiclepositionupdates?'>Metro Trains Vehicle Positions</a><br>\n",
    "\n",
    "To get the best access to these datasets may require the use of an API key. Getting a key is free and has many benefits. The more data you have at your fingertips for analysing, the better, right? Check the Melbourne Open Data and the VicRoads Data Exchange Platform page for details.<br>\n",
    "\n",
    "Other datasets that we use were found through information freely available on the internet which we put into CSV files for easy consumption.\n",
    "\n",
    "As we go through each of the three analytical approaches below, we will show what packages need to be installed and why they are necessary. However, if you are using Conda, we have also provided a 'yml' file which you can use to set up an environment with everything you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\">&emsp;<b>Approach 1: Statistical Analysis of Local Major Events</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "!pip -q install sodapy\n",
    "!pip -q install folium\n",
    "!pip -q install plotly\n",
    "!pip -q install seaborn\n",
    "!pip -q install altair\n",
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from folium.plugins import HeatMapWithTime\n",
    "from folium.plugins import HeatMap\n",
    "import folium.plugins as plugins  \n",
    "from folium import Map\n",
    "from folium import features\n",
    "import altair as alt\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the packages that we have included here will be familiar to you. If you haven't used Socrata before, that is what we use to connect via API to Melbourne City's Open Data Platform.<br>\n",
    "If you haven't had the pleasure of playing with Folium before, then you will probably enjoy learning about it. Folium is a really easy way to produce great looking maps with only a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install folium --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read and view AFL Data\n",
    "afl_df = pd.read_csv('afl_data.csv')\n",
    "afl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardise the datatime data\n",
    "afl_df['startTime'] = pd.to_datetime(afl_df['startTime'], format = '%H:%M')\n",
    "\n",
    "afl_df.date = pd.to_datetime(afl_df.date, format = '%d-%b-%y')\n",
    "afl_df['day_of_week'] = afl_df['date'].dt.day_name()\n",
    "\n",
    "afl_df['StartHour'] = pd.to_datetime(afl_df['startTime'], format='%H:%M:%S').dt.hour\n",
    "afl_df['StartMinute'] = pd.to_datetime(afl_df['startTime'], format='%H:%M:%S').dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate start time categories\n",
    "afl_df['timeslot'] = pd.to_datetime(afl_df['startTime'])\n",
    "afl_df['timeslot'] = afl_df['timeslot'].dt.strftime(\"%H:%M:%S\")\n",
    "afl_df['timeslot'] = afl_df['timeslot'].apply(lambda x: 'Afternoon' if x <= '16:00:00' else 'Evening')\n",
    "afl_df['start'] = afl_df['startTime'].dt.strftime(\"%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract game date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_dates_list = pd.unique((afl_df.date).dt.strftime('%Y-%m-%d')).tolist()\n",
    "\n",
    "afternoons_dates = afl_df[(afl_df.timeslot == 'Afternoon')]\n",
    "afternoons_dates_list = pd.unique((afternoons_dates.date).dt.strftime('%Y-%m-%d')).tolist()\n",
    "\n",
    "evening_dates = afl_df[(afl_df.timeslot == 'Evening')]\n",
    "evening_dates_list = pd.unique((evening_dates.date).dt.strftime('%Y-%m-%d')).tolist()\n",
    "\n",
    "\n",
    "MCG_dates = afl_df[(afl_df.venue == 'M.C.G.')]\n",
    "MCG_dates_list = pd.unique((MCG_dates.date).dt.strftime('%Y-%m-%d')).tolist()\n",
    "\n",
    "Docklands_dates = afl_df[(afl_df.timeslot == 'Docklands')]\n",
    "Docklands_dates_list = pd.unique((Docklands_dates.date).dt.strftime('%Y-%m-%d')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pedestrian data\n",
    "client = Socrata('data.melbourne.vic.gov.au', 'jkWLqYGpmFN5bK6j45TU4peYP', None)\n",
    "results = client.get(\"b2ak-trbp\", limit=7000000)\n",
    "\n",
    "pedestrian_df = pd.DataFrame.from_records(results)\n",
    "#pedestrian_df['sensor_id'] = pedestrian_df['sensor_id'].astype(int)\n",
    "\n",
    "#View pedestrian data\n",
    "pedestrian_df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location data\n",
    "client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "ped_data_location = \"h57g-5234\"\n",
    "results = client.get(ped_data_location)\n",
    "sensor_location = pd.DataFrame.from_records(results)\n",
    "sensor_location[['latitude', 'longitude']] = sensor_location[['latitude', 'longitude']].astype(float)\n",
    "#sensor_location['sensor_id'] = sensor_location['sensor_id'].astype(str)\n",
    "\n",
    "#View sensor data\n",
    "sensor_location.head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise all sensor locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot location of sensors\n",
    "map = folium.Map(location=[sensor_location.latitude.mean(), \n",
    "                           sensor_location.longitude.mean()], \n",
    "                          zoom_start=13.5, min_zoom=13, max_zoom = 16,max_bounds=True)\n",
    "\n",
    "for i in range(0,len(sensor_location)):\n",
    "        label = 'Sensor ID: ' + sensor_location.iloc[i]['sensor_id']\n",
    "        folium.Marker(location = [sensor_location.iloc[i]['latitude'], \n",
    "                            sensor_location.iloc[i]['longitude']], \n",
    "                            popup = label).add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(pedestrian_df, sensor_location, on = 'sensor_id')\n",
    "\n",
    "merged_df = merged_df[['date_time', 'year', 'month', 'mdate', 'day', 'time', 'sensor_id',\n",
    "       'sensor_name_x', 'hourly_counts','latitude',\n",
    "       'longitude']]\n",
    "\n",
    "merged_df['date_time'] = pd.to_datetime(merged_df['date_time'])\n",
    "merged_df['date'] = merged_df['date_time'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "merged_df = merged_df.sort_values(['date_time'], ascending = True)\n",
    "merged_df['hourly_counts'] = merged_df['hourly_counts'].astype(int)\n",
    "merged_df['day'] = merged_df['day'].astype(str)\n",
    "\n",
    "#merged_df['date_time'] = pd.to_datetime(merged_df['date_time'])\n",
    "merged_df['year'] = merged_df['year'].astype(int)\n",
    "merged_df['mdate'] = merged_df['mdate'].astype(int)\n",
    "merged_df['time'] = merged_df['time'].astype(int)\n",
    "merged_df['hourly_counts'] = merged_df['hourly_counts'].astype(int)\n",
    "merged_df['year'] = merged_df['year'].astype(int)\n",
    "merged_df['sensor_id'] = merged_df['sensor_id'].astype(int)\n",
    "all_sensors=list(pd.unique(merged_df['sensor_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust dataframe to AFL data\n",
    "merged_df.reset_index(inplace =True)\n",
    "afl_merged_df = merged_df[merged_df.date > '2012-03-29']\n",
    "\n",
    "merged_df2 = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract game and and non game data\n",
    "afl_df = afl_merged_df[merged_df['date'].isin(game_dates_list)]\n",
    "non_afl_df = afl_merged_df[~merged_df['date'].isin(game_dates_list)]\n",
    "afl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_afl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate average pedestraian movements per hour\n",
    "afl = pd.DataFrame(afl_df.groupby(['sensor_id', 'year', 'day','time', 'latitude', 'longitude'])['hourly_counts'].mean())\n",
    "afl.reset_index(inplace = True)\n",
    "\n",
    "non_afl = pd.DataFrame(non_afl_df.groupby(['sensor_id', 'year', 'day','time', 'latitude', 'longitude'])['hourly_counts'].mean())\n",
    "non_afl.reset_index(inplace = True)\n",
    "\n",
    "#Combine AFL and Non-AFL Data\n",
    "afl_merged_df= non_afl.merge(afl, on=[\"sensor_id\",\"year\", \"day\", \"time\"])\n",
    "afl_merged_df.drop(['latitude_x', 'longitude_x'], axis=1, inplace=True)\n",
    "afl_merged_df = afl_merged_df.rename(columns = {'latitude_y':'latitude','longitude_y':'longitude' })\n",
    "afl_merged_df = afl_merged_df.rename(columns = {'hourly_counts_x':'No_AFL','hourly_counts_y':'AFL' })\n",
    "\n",
    "merged_data_diff = afl_merged_df.copy()\n",
    "merged_data_diff['diff'] = afl_merged_df.AFL - afl_merged_df.No_AFL\n",
    "\n",
    "merged_data_diff = afl_merged_df.copy()\n",
    "merged_data_diff['diff'] = (afl_merged_df.AFL - afl_merged_df.No_AFL)/afl_merged_df.AFL\n",
    "merged_data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_merged_df = afl_merged_df.melt([\"sensor_id\", \"year\", \"day\", \"time\", \"latitude\", \"longitude\"],var_name=\"AFL\",value_name=\"PedestrianCount\")\n",
    "\n",
    "#Check for duplicate dates\n",
    "list1 = afl_df.date_time\n",
    "list2 = non_afl_df.date_time\n",
    "\n",
    "if any(x in list1 for x in list2):\n",
    "    print(\"Duplicates found.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "afl_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plot comparing days with and without an AFL Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot afl v non-afl movmeents, enter sensor_id, year and day of the week\n",
    "def dayPlot(sensor, year, day):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.lineplot(x='time', y='PedestrianCount', hue='AFL', \n",
    "                 data = afl_merged_df[(afl_merged_df.sensor_id == sensor) & \n",
    "                                    (afl_merged_df.year == year) & \n",
    "                                    (afl_merged_df.day == day)]).set_title('Pedestrian variation - AFL Game v No AFL Game')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dayplot -  sensor id, year and day of the week\n",
    "day = dayPlot(7, 2019, 'Saturday')\n",
    "day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise days with and without an AFL Game - Click on Sensor for Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PedestrianVar(year, day, *sensor_ids):\n",
    "  m = folium.Map(location=[sensor_location.latitude.mean(), \n",
    "                            sensor_location.longitude.mean()], \n",
    "                            zoom_start=14, control_scale=True, min_zoom=14, max_zoom = 16)\n",
    "\n",
    "  for i in sensor_ids:\n",
    "    data = afl_merged_df[(afl_merged_df.sensor_id == i) & \n",
    "                                (afl_merged_df.year == year) & \n",
    "                                (afl_merged_df.day == day)]\n",
    "\n",
    "    pedestrian_chart = alt.Chart(data).mark_line().encode(\n",
    "      x='time',\n",
    "      y='PedestrianCount',\n",
    "      color='AFL')\n",
    "    \n",
    "    chart = json.loads(pedestrian_chart.to_json())\n",
    "\n",
    "    popup = folium.Popup(max_width=350)\n",
    "    folium.features.VegaLite(chart, height=200, width=350).add_to(popup)\n",
    "    folium.Marker([data.iloc[i]['latitude'], \n",
    "                              data.iloc[i]['longitude']], tooltip = data.iloc[i]['sensor_id'], popup=popup).add_to(m)\n",
    "\n",
    "  return(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_variance = PedestrianVar(2019, 'Saturday', 9,10,2,6,18,8,11,1,5,12,3,14,31,7,2, 33)\n",
    "pedestrian_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differentiate between start times, before 4pm is Afternoon, after 4pm is considered evening.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract afternoon and evening data\n",
    "afl_afternoon_df = afl_df[afl_df['date'].isin(afternoons_dates_list)]\n",
    "afl_evening_df = afl_df[~afl_df['date'].isin(afternoons_dates_list)]\n",
    "\n",
    "afl_afternoon_df2 = pd.DataFrame(afl_afternoon_df.groupby(['sensor_id', 'year', 'day','time', 'latitude', 'longitude'])['hourly_counts'].mean())\n",
    "afl_afternoon_df2.reset_index(inplace = True)\n",
    "\n",
    "afl_evening_df2 = pd.DataFrame(afl_evening_df.groupby(['sensor_id', 'year', 'day','time','latitude', 'longitude'])['hourly_counts'].mean())\n",
    "afl_evening_df2.reset_index(inplace = True)\n",
    "#non_afl.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "dfs = [afl_evening_df2,afl_afternoon_df2,non_afl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = reduce(lambda  left,right: pd.merge(left,right,on=[\"sensor_id\", \"year\", \"day\", \"time\"],\n",
    "                                            how='outer'), dfs)\n",
    "final_df = final_df.rename(columns = {'hourly_counts_x':'Evening','hourly_counts_y':'Afternoon', 'hourly_counts': 'No AFL'})\n",
    "final_df.drop(['latitude_x', 'longitude_x', 'latitude_y', 'longitude_y'], axis=1, inplace=True)\n",
    "time_merged_data = final_df.melt([\"sensor_id\", \"year\", \"day\", \"time\", \"latitude\", \"longitude\"],var_name=\"AFL\",value_name=\"hourly_counts\")\n",
    "time_merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plot comparing days with different start times and No AFL Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot non-afl v afternoon v evening movements, enter sensor_id, year and day of the week\n",
    "\n",
    "def TimePlot(year, day, sensor):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.lineplot(x='time', y='hourly_counts', hue='AFL', \n",
    "                 data = time_merged_data[(time_merged_data.sensor_id == sensor) & \n",
    "                                    (time_merged_data.year == year) & \n",
    "                                    (time_merged_data.day == day)]).set_title('Pedestrian variation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = TimePlot(2016, 'Sunday', 24)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise variation between games start times - Click on Sensor for Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeVar(year, day, *sensor_ids):\n",
    "\n",
    "  m = folium.Map(location=[sensor_location.latitude.mean(), \n",
    "                            sensor_location.longitude.mean()], \n",
    "                            zoom_start=15, control_scale=False, min_zoom=14, max_zoom = 16)\n",
    "\n",
    "  for i in sensor_ids:\n",
    "    data = time_merged_data[(time_merged_data.sensor_id == i) & \n",
    "                                (time_merged_data.year == 2016) & \n",
    "                                (time_merged_data.day == 'Sunday')]\n",
    "\n",
    "    pedestrian_chart = alt.Chart(data).mark_line().encode(\n",
    "      x='time',\n",
    "      y='hourly_counts',\n",
    "      color='AFL')\n",
    "    \n",
    "    chart = json.loads(pedestrian_chart.to_json())\n",
    "\n",
    "    popup = folium.Popup(max_width=350)\n",
    "    folium.features.VegaLite(chart, height=200, width=350).add_to(popup)\n",
    "    folium.Marker([data.iloc[i]['latitude'], \n",
    "                              data.iloc[i]['longitude']], tooltip = data.iloc[i]['sensor_id'], popup=popup).add_to(m)\n",
    "\n",
    "  return(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeVariance = TimeVar(2020, 'Sunday',7,9,10,2,6,18,8,11,1,5,12,3,14,31)\n",
    "TimeVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differentiate between grounds, MCG or Docklands.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ground data\n",
    "afl_mcg_df = afl_df[afl_df['date'].isin(MCG_dates_list)]\n",
    "afl_docklands_df = afl_df[~afl_df['date'].isin(MCG_dates_list)]\n",
    "\n",
    "afl_mcg_df2 = pd.DataFrame(afl_mcg_df.groupby(['sensor_id', 'year', 'day','time','latitude', 'longitude'])['hourly_counts'].mean())\n",
    "afl_mcg_df2.reset_index(inplace = True)\n",
    "\n",
    "afl_docklands_df2 = pd.DataFrame(afl_docklands_df.groupby(['sensor_id', 'year', 'day','time', 'latitude', 'longitude'])['hourly_counts'].mean())\n",
    "afl_docklands_df2.reset_index(inplace = True)\n",
    "\n",
    "dfs = [afl_mcg_df2,afl_docklands_df2,non_afl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = reduce(lambda  left,right: pd.merge(left,right,on=[\"sensor_id\", \"year\", \"day\", \"time\"],\n",
    "                                            how='outer'), dfs)\n",
    "final_df = final_df.rename(columns = {'hourly_counts_x':'MCG','hourly_counts_y':'Docklands', 'hourly_counts': 'No AFL'})\n",
    "final_df.drop(['latitude_x', 'longitude_x', 'latitude_y', 'longitude_y'], axis=1, inplace=True)\n",
    "time_merged_data = final_df.melt([\"sensor_id\", \"year\", \"day\", \"time\", \"latitude\", \"longitude\"],var_name=\"AFL\",value_name=\"hourly_counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot non-afl v afternoon v evening movements, enter sensor_id, year and day of the week\n",
    "\n",
    "def GroundPlot(sensor, year, day):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.lineplot(x='time', y='hourly_counts', hue='AFL', \n",
    "                 data = time_merged_data[(time_merged_data.sensor_id == sensor) & \n",
    "                                    (time_merged_data.year == year) & \n",
    "                                    (time_merged_data.day == day)]).set_title('Pedestrian variation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plot comparing days with games at different grounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroundVar = GroundPlot(23, 2017, 'Sunday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of games at different ground and no AFL games - Click on Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroundVar(year, day, *sensor_ids):\n",
    "  m = folium.Map(location=[sensor_location.latitude.mean(), \n",
    "                            sensor_location.longitude.mean()], \n",
    "                            zoom_start=13, control_scale=True, min_zoom=14, max_zoom = 16)\n",
    "\n",
    "  for i in sensor_ids:\n",
    "    data = time_merged_data[(time_merged_data.sensor_id == i) & \n",
    "                                (time_merged_data.year == year) & \n",
    "                                (time_merged_data.day == day)]\n",
    "\n",
    "    pedestrian_chart = alt.Chart(data).mark_line().encode(\n",
    "      x='time',\n",
    "      y='hourly_counts',\n",
    "      color='AFL')\n",
    "    \n",
    "    chart = json.loads(pedestrian_chart.to_json())\n",
    "\n",
    "    popup = folium.Popup(max_width=350)\n",
    "    folium.features.VegaLite(chart, height=200, width=350).add_to(popup)\n",
    "    folium.Marker([data.iloc[i]['latitude'], \n",
    "                              data.iloc[i]['longitude']], tooltip = data.iloc[i]['sensor_id'], popup=popup).add_to(m)\n",
    "\n",
    "  return(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroundVariance = GroundVar(2016, 'Sunday',7,9,10,2,6,18,8,11,1,5,12,3,14,31,13)\n",
    "GroundVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise HeatMap - Indicating how far above the avearge for that particular time and day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VarHeatMap(year, day):\n",
    "    merged_data_diff2 = merged_data_diff[(merged_data_diff.year == year) & (merged_data_diff.day == day)]\n",
    "    data = []\n",
    "    for _, d in merged_data_diff2.groupby('time'):\n",
    "        data.append([[row['latitude'], row['longitude'], row['diff']] for _, row in d.iterrows()])\n",
    "\n",
    "    time_index = [k[0] for k in merged_data_diff2.groupby('time')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    map= folium.Map(location=[sensor_location.latitude.mean(), \n",
    "                               sensor_location.longitude.mean()],\n",
    "                                zoom_start=14)\n",
    "\n",
    "\n",
    "    hm = HeatMapWithTime(data, index = time_index, name = 'Heat Map',auto_play=True, \n",
    "                    min_opacity=.5, \n",
    "                    gradient = {0.05: 'blue', \n",
    "                                0.1: 'green', \n",
    "                                0.3: 'orange', \n",
    "                                0.45: 'red'}).add_to(map)\n",
    "    return(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarHeatMap(2016, 'Sunday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce new Data - Melbourne Grand Prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_dates = pd.read_csv('gp_dates.csv' )\n",
    "gp_dates['date'] = pd.to_datetime(gp_dates['date'])\n",
    "\n",
    "gp_dates = gp_dates[gp_dates.date> '1999-01-01']\n",
    "gp_dates_list = pd.unique((gp_dates.date).dt.strftime('%Y-%m-%d')).tolist()\n",
    "\n",
    "gp_df = merged_df[merged_df['date'].isin(gp_dates_list)]\n",
    "non_gp_df = merged_df[~merged_df['date'].isin(gp_dates_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate average pedestrian movements per hour\n",
    "gp = pd.DataFrame(gp_df.groupby(['sensor_id', 'year', 'day','time', 'latitude', 'longitude'])['hourly_counts'].mean())\n",
    "gp.reset_index(inplace = True)\n",
    "\n",
    "non_gp = pd.DataFrame(non_gp_df.groupby(['sensor_id', 'year', 'day','time', 'latitude', 'longitude'])['hourly_counts'].mean())\n",
    "non_gp.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Grand Prix and Non-Grand Prix Data\n",
    "gp_merged_df= non_gp.merge(gp, on=[\"sensor_id\",\"year\", \"day\", \"time\"])\n",
    "gp_merged_df.drop(['latitude_x', 'longitude_x'], axis=1, inplace=True)\n",
    "gp_merged_df = gp_merged_df.rename(columns = {'latitude_y':'latitude','longitude_y':'longitude' })\n",
    "gp_merged_df = gp_merged_df.rename(columns = {'hourly_counts_x':'No_GP','hourly_counts_y':'GP' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_merged_df = gp_merged_df.rename(columns = {'hourly_counts_x':'No_GP','hourly_counts_y':'GP' })\n",
    "gp_merged_data_diff = gp_merged_df.copy()\n",
    "gp_merged_data_diff['diff'] = (gp_merged_data_diff.GP - gp_merged_data_diff.No_GP)/gp_merged_data_diff.GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_merged_df = gp_merged_df.melt([\"sensor_id\", \"year\", \"day\", \"time\", \"latitude\", \"longitude\"],var_name=\"GP\",value_name=\"PedestrianCount\")\n",
    "gp_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plot comparing days with days with Grand Prix and without (must be Friday, Saturday or Sunday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot grand prix v non-grand movmements, enter sensor_id, year and day of the week\n",
    "def GP_Plot(sensor, year, day):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.lineplot(x='time', y='PedestrianCount', hue='GP', \n",
    "                 data = gp_merged_df[(gp_merged_df.sensor_id == sensor) & \n",
    "                                    (gp_merged_df.year == year) & \n",
    "                                    (gp_merged_df.day == day)]).set_title('Pedestrian variation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPVariance = GP_Plot(31, 2019, 'Saturday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of days with and without Grand Prix (Friday, Saturday or Sunday)- Click on Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPPedestrianVar(year, day, *sensor_ids):\n",
    "  m = folium.Map(location=[sensor_location.latitude.mean(), \n",
    "                            sensor_location.longitude.mean()], \n",
    "                            zoom_start=14, control_scale=True, min_zoom=14, max_zoom = 16)\n",
    "\n",
    "  for i in sensor_ids:\n",
    "    data = gp_merged_df[(gp_merged_df.sensor_id == i) & \n",
    "                                (gp_merged_df.year == year) & \n",
    "                                (gp_merged_df.day == day)]\n",
    "\n",
    "    pedestrian_chart = alt.Chart(data).mark_line().encode(\n",
    "      x='time',\n",
    "      y='PedestrianCount',\n",
    "      color='GP')\n",
    "    \n",
    "    chart = json.loads(pedestrian_chart.to_json())\n",
    "\n",
    "    popup = folium.Popup(max_width=350)\n",
    "    folium.features.VegaLite(chart, height=200, width=350).add_to(popup)\n",
    "    folium.Marker([data.iloc[i]['latitude'], \n",
    "                              data.iloc[i]['longitude']], tooltip = data.iloc[i]['sensor_id'], popup=popup).add_to(m)\n",
    "\n",
    "  return(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_variance = GPPedestrianVar(2018, 'Sunday',7,9,10,2,6,18,8,11,1,5,12,3,14,31,44)\n",
    "pedestrian_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\">&emsp;<b>Approach 2: Visual Analysis of Real-Time Train Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import BeautifyIcon\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the real time data provided by the VicRoads open data platform, you will need to use 'gtfs_realtime_pb2'. This is a package developed to work with the GTFS (General Transit Feed Specification) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Ocp-Apim-Subscription-Key': '46f44fa970e44e04be413233229d3c09',}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "feed = gtfs_realtime_pb2.FeedMessage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicroads planned closure API\n",
    "### Services Alerts\n",
    "Cancellation of metro train trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = http.client.HTTPSConnection('data-exchange-api.vicroads.vic.gov.au')\n",
    "    conn.request(\"GET\", \"/opendata/v1/gtfsr/metrotrain-servicealerts?%s\" % params, \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    feed.ParseFromString(data)\n",
    "    print(feed)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Updates\n",
    "The trip update feed provides real-time arrival and departure information of a trip where data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = http.client.HTTPSConnection('data-exchange-api.vicroads.vic.gov.au')\n",
    "    conn.request(\"GET\", \"/opendata/v1/gtfsr/metrotrain-tripupdates?%s\" % params, \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    feed.ParseFromString(data)\n",
    "    print(feed)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_list = []\n",
    "stop_time_list = []\n",
    "\n",
    "for entity in feed.entity:\n",
    "  if entity.HasField('trip_update'):\n",
    "      trip_update = entity.trip_update.trip\n",
    "      trip_list.append(\n",
    "          {'trip_id': trip_update.trip_id,\n",
    "           'start_time': trip_update.start_time,\n",
    "           'start_date': trip_update.start_date\n",
    "           })\n",
    "\n",
    "      stop_times = entity.trip_update.stop_time_update\n",
    "      for st in stop_times:\n",
    "        stop_time_list.append(\n",
    "          {'trip_id': trip_update.trip_id,\n",
    "           'stop_sequence': st.stop_sequence,\n",
    "           'arrival_time': st.arrival.time,\n",
    "           'depart_time': st.departure.time,\n",
    "           })\n",
    "\n",
    "\n",
    "df_trip_update = pd.DataFrame(trip_list, columns = ['trip_id','start_time','start_date'])\n",
    "df_stop_time_update = pd.DataFrame(stop_time_list, columns = ['trip_id','stop_sequence','arrival_time','depart_time'])\n",
    "\n",
    "# Clean data\n",
    "df_trip_update['start_date'] = pd.to_datetime(df_trip_update['start_date'], format='%Y%m%d')\n",
    "df_stop_time_update['arrival_time'] = pd.to_datetime(df_stop_time_update['arrival_time'], unit='s')\n",
    "df_stop_time_update['depart_time'] = pd.to_datetime(df_stop_time_update['depart_time'], unit='s')\n",
    "\n",
    "display(df_trip_update)\n",
    "display(df_stop_time_update.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Positions\n",
    "The vehicle position feed contains live location and occupancy of the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = http.client.HTTPSConnection('data-exchange-api.vicroads.vic.gov.au')\n",
    "    conn.request(\"GET\", \"/opendata/v1/gtfsr/metrotrain-vehicleposition-updates?%s\" % params, \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    feed.ParseFromString(data)\n",
    "    print(feed)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_list = []\n",
    "\n",
    "for entity in feed.entity:\n",
    "  if entity.HasField('vehicle'):\n",
    "      trip_update = entity.vehicle.trip\n",
    "      position = entity.vehicle.position\n",
    "      location_list.append(\n",
    "          {'trip_id': trip_update.trip_id,\n",
    "           'start_time': trip_update.start_time,\n",
    "           'start_date': trip_update.start_date,\n",
    "           'lat': position.latitude,\n",
    "           'lon': position.longitude,\n",
    "           'bearing': position.bearing,\n",
    "           'timestamp': entity.vehicle.timestamp,\n",
    "           'vehicle_id': entity.vehicle.vehicle.id,\n",
    "           'occupancy_stat': entity.vehicle.occupancy_status\n",
    "           })\n",
    "\n",
    "df_vehicle_location = pd.DataFrame(location_list, columns = ['trip_id', 'start_time', 'start_date', 'lat', 'lon', 'bearing', 'timestamp', 'vehicle_id', 'occupancy_stat'])\n",
    "\n",
    "# Clean data\n",
    "df_vehicle_location['start_date'] = pd.to_datetime(df_vehicle_location['start_date'], format='%Y%m%d')\n",
    "df_vehicle_location['timestamp'] = pd.to_datetime(df_vehicle_location['timestamp'], unit='s')\n",
    "df_vehicle_location['timestamp'] = df_vehicle_location['timestamp'].dt.tz_localize('UTC').dt.tz_convert('Australia/Sydney')\n",
    "\n",
    "display(df_vehicle_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on occupancy_stat**\n",
    "0 : EMPTY\n",
    "1 : MANY_SEATS_AVAILABLE\n",
    "2 : FEW_SEATS_AVAILABLE\n",
    "3 : STANDING_ROOM_ONLY\n",
    "...\n",
    "\n",
    "Ref: https://developers.google.com/transit/gtfs-realtime/reference#enum-occupancystatus\n",
    "\n",
    "#### Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare colour dictionary for occupancy level\n",
    "keys = list(df_vehicle_location['occupancy_stat'].unique())\n",
    "color_range = list(np.linspace(0, 1, len(keys), endpoint=False))\n",
    "colors = [matplotlib.colors.to_hex(plt.cm.Reds(x)) for x in color_range]\n",
    "color_dict_industry = dict(zip(keys, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle Occupancy Status\n",
    "train_occp = folium.FeatureGroup(name=\"Train Occupancy Status\",\n",
    "                                show=True,)\n",
    "\n",
    "\n",
    "for i in range(0,len(df_vehicle_location)):\n",
    "\n",
    "  # styles = {\n",
    "  #   'fill': True,\n",
    "  #   'color': color_dict_industry[df_vehicle_location.iloc[i]['occupancy_stat']],\n",
    "  #   'weight': 1.5,\n",
    "  #   # 'fillOpacity': 1\n",
    "  # }\n",
    "\n",
    "  icon = BeautifyIcon(\n",
    "    icon='arrow-up',\n",
    "    background_color=color_dict_industry[df_vehicle_location.iloc[i]['occupancy_stat']],\n",
    "    # icon_shape='marker',\n",
    "    inner_icon_style=f'transform: rotate({df_vehicle_location.iloc[i][\"bearing\"]}deg);'\n",
    "  )\n",
    "\n",
    "  html=f\"\"\"\n",
    "      <h5>{df_vehicle_location.iloc[i]['trip_id']}</h5>\n",
    "      <p>Occupancy Status: {df_vehicle_location.iloc[i]['occupancy_stat']}</p>\n",
    "      <p>Start Time: {df_vehicle_location.iloc[i]['start_time']}</p>\n",
    "      \"\"\"\n",
    "  iframe = folium.IFrame(html=html, width=200, height=200)\n",
    "  popup = folium.Popup(iframe, max_width=2650)\n",
    "\n",
    "  folium.Marker(\n",
    "    location=[df_vehicle_location.iloc[i]['lat'], df_vehicle_location.iloc[i]['lon']],\n",
    "    popup=popup,\n",
    "    # radius=float(df_vehicle_location.iloc[i]['occupancy_stat'])*100,\n",
    "    icon=icon,\n",
    "    # **styles\n",
    "  ).add_to(train_occp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = folium.Map(location=[-37.813, 144.945], tiles=\"CartoDB dark_matter\", zoom_start=13)\n",
    "\n",
    "train_occp.add_to(map)\n",
    "folium.LayerControl(collapsed=False).add_to(map)\n",
    "\n",
    "# Show the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:white; background-color: black\">&emsp;<b>Approach 3: Predicitve Linear Models and Interactive Visualisation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these packages will be familiar from reading through the two earlier approaches taken in this notebook.\n",
    "To do the regression modelling, several imports are used from the popular Scikit-Learn package.<br>\n",
    "To create the interactive visualisation, Dash and several of the related components of Dash are needed.<br>\n",
    "*Note - Dash runs in a browser. You can test it on a local browser before deploying it. However, if you want to run the interface within the notebook, you can install JupyterDash instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3, Part 1: Investigating the Pedestrian Sensor Data.\n",
    "### Pedestrian sensor data - basic analysis.\n",
    "\n",
    "This next dataset is pulled from the Melbourne Open Playground. The code block in the next cell needs to be uncommented to extract the data and copy it into a csv file that gets stored on your local drive.\n",
    "\n",
    "This section of Approach 3 will see whether the data in this dataset can be used to make predictions of the pedestrian counts.\n",
    "\n",
    "Further sections will introduce new datasets and see if the extra information can help to make better, more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the below to open data source, download sensor data, and store it as a csv locally.\n",
    "\n",
    "##Function to get Sensor count history data\n",
    "# def sensor_count():\n",
    "#     client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "#     sensor_data_id = \"b2ak-trbp\"\n",
    "#     results = client.get(sensor_data_id, limit=5000000)\n",
    "#     df = pd.DataFrame.from_records(results)\n",
    "#     df = df[['year', 'month', 'mdate', 'day', 'time', 'sensor_id', 'sensor_name', 'hourly_counts']]\n",
    "#     return df\n",
    "\n",
    "# sensor_history = sensor_count()\n",
    "\n",
    "# sensor_history.to_csv('sensor_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_history = pd.read_csv('sensor_history.csv')\n",
    "\n",
    "#This function grabs the location (longitude and latitude) of the pedestrian sensors\n",
    "def sensor_location():\n",
    "    client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "    sensor_location_data_id = \"h57g-5234\"\n",
    "    results = client.get(sensor_location_data_id)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    sensor_location = df[[\"sensor_id\", \"sensor_description\", \"latitude\", \"longitude\"]]\n",
    "    sensor_location.columns = [\"Sensor ID\", \"Sensor Description\", \"lat\", \"lon\"]\n",
    "    sensor_location[\"lat\"] = sensor_location[\"lat\"].apply(lambda x: float(x))\n",
    "    sensor_location[\"lon\"] = sensor_location[\"lon\"].apply(lambda x: float(x))\n",
    "    return sensor_location\n",
    "\n",
    "sensor_location = sensor_location()\n",
    "sensor_location['sensor_id'] = sensor_location['Sensor ID'].astype(int)\n",
    "sensor_location = sensor_location.drop(['Sensor ID', 'Sensor Description'], axis=1)\n",
    "sensor_history = sensor_history.merge(sensor_location, on=('sensor_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a bit of a look at the dataset.\n",
    "print(sensor_history.head())\n",
    "print(\"\")\n",
    "print(sensor_history.info())\n",
    "print(\"\")\n",
    "print(sensor_history.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a quick linear regression to see how well we can model the relationships\n",
    "#contained within the pedestrian sensor network data.\n",
    "x = sensor_history.drop(columns='hourly_counts')\n",
    "y = sensor_history.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "X = X.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3, Part 2: Adding new datasets:\n",
    "### Add a new dataset: climate microsensors.\n",
    "\n",
    "We didn't get a great result from the previous model. The score it output is the 'R-squared' score. These scores range from 0 to 1, with 1 being a perfect score and 0 being the worst possible score.\n",
    "\n",
    "Let's see if we can improve on this score by adding other datasets.\n",
    "\n",
    "This next dataset is also pulled from the Melbourne Open Playground. The code block in the next cell needs to be uncommented to extract the data and copy it into a csv file that gets stored on your local drive.\n",
    "\n",
    "The dataset is based on climate microsensors in Melbourne's CBD. For this analysis, we are only trying to get an idea of what the climate is like in Melbourne's city as a whole, not going into the detail of each sensor location. So we are only grabbing the data from one sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to get Sensor count history data\n",
    "# def micro_count():\n",
    "#     client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "#     micro_data_id = \"u4vh-84j8\"\n",
    "#     results = client.get(micro_data_id, limit=4000000)\n",
    "#     if results:\n",
    "#         df = pd.DataFrame.from_records(results)\n",
    "#     return df\n",
    "\n",
    "# micro_history = micro_count()\n",
    "\n",
    "# micro_history.to_csv('micro_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_history = pd.read_csv('micro_history.csv')\n",
    "\n",
    "micro_history = micro_history[(micro_history.sensor_id == '5a') | (micro_history.sensor_id == '5b') |\n",
    "                             (micro_history.sensor_id == '5c') | (micro_history.sensor_id == '0a') |\n",
    "                             (micro_history.sensor_id == '0b') | (micro_history.sensor_id == '6')]\n",
    "\n",
    "micro_history = micro_history[(micro_history.site_id == 1003) | (micro_history.site_id == 1009)]\n",
    "\n",
    "micro_history = micro_history.drop(['id', 'gateway_hub_id', 'type', 'units'], axis=1)\n",
    "\n",
    "micro_history.loc[micro_history.sensor_id == '5a', 'temp'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '5b', 'humidity'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '5c', 'pressure'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '0a', 'part_2p5'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '0b', 'part_10'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '6', 'wind'] = micro_history.value\n",
    "\n",
    "micro_history.local_time = pd.to_datetime(micro_history.local_time, format='%Y-%m-%d')\n",
    "micro_history['year'] = micro_history.local_time.dt.year\n",
    "micro_history['month'] = micro_history.local_time.dt.month_name()\n",
    "micro_history['mdate'] = micro_history.local_time.dt.day\n",
    "micro_history['time'] = micro_history.local_time.dt.hour\n",
    "\n",
    "micro_history = micro_history.drop(['site_id', 'sensor_id', 'value', 'local_time'], axis=1)\n",
    "micro_history = micro_history.groupby(by=['year', 'month', 'mdate', 'time']).max()\n",
    "\n",
    "ped_climate = sensor_history.merge(micro_history, on=('year', 'month', 'mdate', 'time'), how='inner')\n",
    "\n",
    "x = ped_climate.drop(columns='hourly_counts')\n",
    "y = ped_climate.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "X = X.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a different dataset: school and public holidays:\n",
    "\n",
    "Adding in the climate data resulted in an imrpovement of the R-squared score. Let's see if we can find other datasets to add in to the model and get that score even higher. The next dataset was one that was created manually - by looking up the details online, then entering them into a csv.\n",
    "You will need to have this csv downloaded into your local directory for this to work.\n",
    "\n",
    "This dataset has details of which dates are public holidays and which are school holidays. This could have an impact on how many people are walking around, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_holidays = pd.read_csv('vic_holidays.csv')\n",
    "\n",
    "ped_holidays = sensor_history.merge(vic_holidays, on=('year', 'month', 'mdate'), how='left')\n",
    "ped_holidays =  ped_holidays.fillna(0)\n",
    "\n",
    "x = ped_holidays.drop(columns='hourly_counts')\n",
    "y = ped_holidays.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about Covid-19?:\n",
    "\n",
    "The holiday dataset only had a small, but consistent, positive effect on the scoring. Let's keep looking. The next dataset was found on the internet. You will need to have this csv downloaded into your local directory for this to work.\n",
    "\n",
    "It contains statistics about covid-19, including historical data. It is expected that these numbers could have a reasonably large impact on the numbers of pedestrians.\n",
    "\n",
    "**Source:** https://govtstats.covid19nearme.com.au/data/all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = pd.read_csv('covid_data.csv')\n",
    "\n",
    "covid_data = covid_data[['DATE', 'VIC_CASES_LOCAL_LAST_24H', 'VIC_CASES_ACTIVE', \n",
    "                    'VIC_CASES_LOCAL_LAST_7D', 'VIC_CASES_OVERSEAS_ACQUIRED_LAST_24H', 'VIC_CASES_OVERSEAS_ACQUIRED_LAST_7D',\n",
    "                    'VIC_CASES_UNDER_INVESTIGATION_LAST_24H', 'VIC_CASES_UNDER_INVESTIGATION_LAST_7D',\n",
    "                    'VIC_TESTS_LAST_7D', 'VIC_TESTS_PER_100K_LAST_7D']]\n",
    "\n",
    "covid_data.fillna(0)\n",
    "\n",
    "covid_data.DATE = pd.to_datetime(covid_data.DATE, format='%Y-%m-%d')\n",
    "\n",
    "covid_data['year'] = covid_data.DATE.dt.year\n",
    "covid_data['month'] = covid_data.DATE.dt.month_name()\n",
    "covid_data['mdate'] = covid_data.DATE.dt.day\n",
    "covid_data['mon'] = covid_data.DATE.dt.month\n",
    "\n",
    "sensor_covid = sensor_history.merge(covid_data, on=('year', 'month', 'mdate'), how='inner')\n",
    "\n",
    "x = sensor_covid.drop(columns='hourly_counts')\n",
    "y = sensor_covid.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id', 'mon', 'DATE'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "#Let's have a look at the newly created dataset:\n",
    "print(sensor_covid.head())\n",
    "print(\"\")\n",
    "print(sensor_covid.corr())\n",
    "\n",
    "#And then do the regression.\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3, Part 3: Making the final dataset:\n",
    "\n",
    "The Covid data also seems to have a small, positive impact on the scoring. What if we add all of these datasets together? Will the sum of the parts be greater, or will the different datasets just confuse the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = ped_climate.merge(sensor_covid, on=('year', 'month', 'mdate', 'day', 'time', 'sensor_id'\n",
    "                                            , 'sensor_name', 'hourly_counts'), how='inner')\n",
    "\n",
    "merged = merge_1.merge(ped_holidays, on=('year', 'month', 'mdate', 'day', 'time', 'sensor_id'\n",
    "                                            , 'sensor_name', 'hourly_counts'), how='inner')\n",
    "\n",
    "merge_days = pd.get_dummies(merged.day)\n",
    "merge_months = pd.get_dummies(merged.month)\n",
    "merge_sensor = pd.get_dummies(merged.sensor_name)\n",
    "merge_drop = merged.drop(['month', 'day', 'sensor_name', 'sensor_id', 'DATE'], axis=1)\n",
    "\n",
    "merged_final = pd.concat([merge_drop, merge_days, merge_months, merge_sensor],axis=1)\n",
    "\n",
    "X = merged_final.drop(columns='hourly_counts')\n",
    "y = merged_final.hourly_counts\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "#And then do the regression.\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3, Part 4: Creating the predictive models:\n",
    "\n",
    "Ok, so now we have some datasets and we have had a look at their individual impacts - before finding that they are stronger when combined together. We have a better understanding of how different events effect the number of pedestrians going past different sensors.\n",
    "\n",
    "But are we using the best model? There are other alternatives such as Decision Tree regressors, Random Forest regressors, Support Vector Machine regressors and even Deep Learning regression models.\n",
    "\n",
    "However, Support Vector Machine regressors can take a long time to run when the data has many dimensions, and Deep Learning models are also resource intensive. Below we will limit ourselves to adding in a Decision Tree regressor and a Random Forest regressor. Even these can take a long time to run, but the results will hopefully be worth it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "LR = LinearRegression(fit_intercept=False)\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The basic Linear Regression R-squared score: \", LR.score(X_test, y_test))\n",
    "\n",
    "#Decision Tree Regressor\n",
    "DT = DecisionTreeRegressor(max_depth = 75)\n",
    "DT.fit(X_train, y_train)\n",
    "print(\"The Decision Tree regressor's R-squared score: \", DT.score(X_test, y_test))\n",
    "\n",
    "#Random Forest Regressor\n",
    "RFR = RandomForestRegressor(n_estimators=150, max_depth=100, n_jobs= -1, max_features=100)\n",
    "RFR.fit(X_train, y_train)\n",
    "print(\"The Random Forest regressor's R-squared score: ', RFR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3, Part 5: Interacting with our predictive models:\n",
    "\n",
    "The R-squared scores we have managed to create now are much better, with the Decision Tree being a huge jump over the basic Linear Regression, and the Random Forest being even better again.\n",
    "\n",
    "So now we have these cool models, we need a really easy, intuitive way to investigate them. For that, we build an interactive interface using Plotly Dash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(external_stylesheets=[dbc.themes.SOLAR])\n",
    "\n",
    "fig = px.scatter_mapbox(merged_final, lat=merged_final.lat, lon=merged_final.lon\n",
    "                        , zoom = 12.5\n",
    "                       , size = merged_final.hourly_counts)\n",
    "fig.update_layout(mapbox_style=\"carto-positron\", mapbox_center_lon=144.96\n",
    "                      , mapbox_center_lat = -37.81)\n",
    "fig.update_layout(margin={\"r\":5,\"t\":5,\"l\":5,\"b\":5})\n",
    "\n",
    "app.layout = html.Div(id='parent', children=[ #main Div\n",
    "    html.Div(id='header', children=[ #header Div\n",
    "        \n",
    "        html.Div([ #calender selector Div\n",
    "            html.P('Choose a date for analysis:'),\n",
    "            dcc.DatePickerSingle(\n",
    "                id = 'selector_date',\n",
    "                month_format = 'MMMM Y',\n",
    "                calendar_orientation = 'horizontal',\n",
    "                placeholder = 'Select a date',\n",
    "                date = date(2021, 6, 21),\n",
    "                display_format = 'DD/MM/YYYY')\n",
    "        ],\n",
    "        style={'width': '15%', 'display': 'inline-block', 'verticalAlign': 'top', 'padding': '20px 20px 20px 20px'}),\n",
    "\n",
    "        html.Div([ #hour slider Div\n",
    "            html.P('Select the hour of the day:'),\n",
    "            dcc.Slider(\n",
    "                id='selector_hour',\n",
    "                min=0,\n",
    "                max=23,\n",
    "                step=1,\n",
    "                value=0,\n",
    "                marks={0: 'midnight', 3: '3am', 6: '6am', 9: '9am', 12: 'midday',\n",
    "                       15: '3pm', 18: '6pm', 21: '9pm'}\n",
    "        )], style={'width': '85%', 'display': 'inline-block', 'textAlign':'left', 'verticalAlign': 'top', 'padding': '20px 20px 20px 20px'}),\n",
    "    ]), #end of 'header' Div\n",
    "\n",
    "    html.Div([ #map and various selectors Div\n",
    "        \n",
    "            html.Div([ #various selectors Div\n",
    "\n",
    "                html.Hr(),\n",
    "                html.P('Temperature:'),\n",
    "                dcc.Slider(\n",
    "                    id='selector_temp',\n",
    "                    min=0,\n",
    "                    max=50,\n",
    "                    value=25,\n",
    "                    marks = {0: '0C', 10: '10C', 20: '20C', 30: '30C', 40: '40C', 50: '50C'}\n",
    "                    ),\n",
    "                html.P('Humidity:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_humid',\n",
    "                    min=0,\n",
    "                    max=100,\n",
    "                    value=0,\n",
    "                    marks = {0: '0%', 25: '25%', 50: '50%', 75: '75%', 100: '100%'}\n",
    "                    ),\n",
    "                html.P('Wind speed:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_wind',\n",
    "                    min=0,\n",
    "                    max=100,\n",
    "                    value=0,\n",
    "                    marks = {0: 'Calm', 20: '20km/h', 40: '40km/h', 60: '60km/h', 80: '80km/h', 100: '100km/h'}\n",
    "                    ),\n",
    "                html.P('Air pressure:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_pressure',\n",
    "                    min=975,\n",
    "                    max=1050,\n",
    "                    value=975,\n",
    "                    marks = {975: '975hPa', 1000: '1000hPa', 1025: '1025hPa', 1050: '1050hPa'}\n",
    "                    ),\n",
    "                html.P('Particulate concentration 2.5 microns:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_part2p5',\n",
    "                    min=0,\n",
    "                    max=500,\n",
    "                    value=0,\n",
    "                    marks = {0: '0', 100: '100', 200: '200', 300: '300', 400: '400', 500: '500'}\n",
    "                    ),\n",
    "                html.P('Particulate concentration 10 microns:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_part10',\n",
    "                    min=0,\n",
    "                    max=1000,\n",
    "                    value=0,\n",
    "                    marks = {0: '0', 250: '200', 500: '500', 750: '750', 1000: '1000'}\n",
    "                    ),\n",
    "                html.Hr(),\n",
    "                html.P('Holiday type:'),\n",
    "                dcc.RadioItems(id='selector_holiday', \n",
    "                   options=[\n",
    "                       {'label': 'School holiday ', 'value': 'SCH'},\n",
    "                       {'label': 'Public holiday ', 'value': 'PUB'},\n",
    "                       {'label': 'Both ', 'value': 'BOTH'},\n",
    "                       {'label': 'Neither ', 'value': 'NONE'}\n",
    "                   ],\n",
    "                   value='NONE'\n",
    "                ),\n",
    "                html.Hr(),\n",
    "\n",
    "                html.P('Covid cases under investigation in the previous 7 days:'),\n",
    "                dcc.Input(id='selector_covid', type='number', min=0, max=10000, step=100, value=0),\n",
    "\n",
    "            ],\n",
    "            style={'height': '49%', 'width': '49%', 'display': 'inline-block', 'padding': '20px 20px 20px 20px'}), #various selectors Div\n",
    "\n",
    "            html.Div(id = 'right_panel', children=[ #format and place the right side panel\n",
    "\n",
    "            html.Div(id='map', children=[ #map div\n",
    "                    dcc.Graph(id = 'world_map', figure = fig)\n",
    "            ],\n",
    "                    style={'textAlign':'center', 'verticalAlign': 'top', 'display': 'inline-block', 'padding': '0px 20px 20px 50px'}), #map selector Div\n",
    "            html.Br(),\n",
    "            html.Hr(),\n",
    "            html.P('Select which predictor model to use:'),\n",
    "            html.Div(id='predictor', children=[ #prediction model div\n",
    "                    dcc.Dropdown(id = 'selector_model', #pick the predictive model to use\n",
    "                        options = [\n",
    "                            {'label':'Linear Regression', 'value':'LR' },\n",
    "                            {'label': 'Decision Tree Regression', 'value':'DT'},\n",
    "                            {'label': 'Random Forest Regression', 'value':'RFR'},\n",
    "                        ],\n",
    "                        value = 'LR'),                        \n",
    "            ],\n",
    "                    style={'textAlign':'center', 'width': '35%', 'display': 'inline-block'}), \n",
    "\n",
    "        ], style={'textAlign':'center', 'width': '49%', 'display': 'inline-block'})\n",
    "    ]) #end of 'map and various selectors' Div\n",
    "]) #end of 'parent' Div\n",
    "\n",
    "@app.callback(Output(component_id='world_map', component_property='figure'),\n",
    "            Input(component_id='selector_temp', component_property='value'),\n",
    "            Input(component_id='selector_humid', component_property='value'),\n",
    "            Input(component_id='selector_wind', component_property='value'),\n",
    "            Input(component_id='selector_part2p5', component_property='value'),\n",
    "            Input(component_id='selector_part10', component_property='value'),\n",
    "            Input(component_id='selector_pressure', component_property='value'),\n",
    "            Input(component_id='selector_holiday', component_property='value'),\n",
    "            Input(component_id='selector_covid', component_property='value'),\n",
    "            Input(component_id='selector_date', component_property='date'),\n",
    "            Input(component_id='selector_hour', component_property='value'),\n",
    "            Input(component_id='selector_model', component_property='value'))\n",
    "\n",
    "def selectors(temp, humid, wind, part_2p5, part_10, pressure, holiday, covid, indate, time, model):\n",
    "\n",
    "    date_object = date.fromisoformat(indate)\n",
    "    year = date_object.year\n",
    "    mon = date_object.month\n",
    "    mdate = date_object.day\n",
    "\n",
    "    scenario = merged_final[(merged_final.year == year) & (merged_final.mdate == mdate)\n",
    "                    & (merged_final.mon == mon) & (merged_final.time == time)]\n",
    "\n",
    "    scenario.school_hol = 0\n",
    "    scenario.pub_hol = 0\n",
    "    \n",
    "    if holiday == 'BOTH':\n",
    "        scenario.school_hol = 1\n",
    "        scenario.pub_hol = 1\n",
    "    if holiday == 'SCH':\n",
    "        scenario.school_hol = 1\n",
    "    if holiday == 'PUB':\n",
    "        scenario.pub_hol = 1\n",
    "    \n",
    "    scenario.temp = temp\n",
    "    scenario.humidity = humid\n",
    "    scenario.wind = wind\n",
    "    scenario.time = time\n",
    "    scenario.pressure = pressure\n",
    "    scenario.part_2p5 = part_2p5\n",
    "    scenario.part_10 = part_10\n",
    "    scenario.VIC_CASES_UNDER_INVESTIGATION_LAST_7D = covid\n",
    "   \n",
    "    if model == 'LR':\n",
    "        guess = LR.predict(scenario.drop(columns='hourly_counts'))\n",
    "    if model == 'DT':\n",
    "        guess = DT.predict(scenario.drop(columns='hourly_counts'))\n",
    "    if model == 'RFR':\n",
    "        guess = RFR.predict(scenario.drop(columns='hourly_counts'))\n",
    "    \n",
    "    scenario.guess = guess.astype(int)\n",
    "\n",
    "    fig = px.scatter_mapbox(scenario, lat=scenario.lat, size = scenario.guess, lon=scenario.lon)\n",
    "    fig.update_layout(mapbox_style=\"carto-positron\", uirevision='scenario')\n",
    "    fig.update_layout(margin={\"r\":5,\"t\":5,\"l\":5,\"b\":5})\n",
    "\n",
    "    return fig\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3, Conclusion:\n",
    "So there we have it. An awesome, fun way to interact with our predictive models!\n",
    "\n",
    "Of course, these models and their predictions can always be improved. You can find new datasets to add, or you can look to make the models better through feature selection, feature reduction or feature engineering. You can also find different ways to for users to interact with the data. The possibilities are endless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: helvetica,arial,sans-serif; font-size:1.6em;color:black; background-color: #EEEEEE\">&emsp;<b>Congratulations. You now know everything about the pedestrian data!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, maybe not everything... of course, these explorations and approaches are just scratching the surface.\n",
    "\n",
    "If you would like to extend this analysis further, please visit the __[City of Melbourne Open Data Site](https://data.melbourne.vic.gov.au/)__ and explore some of the other valuable datasets.\n",
    "\n",
    "Trying to model human behaviour (such as pedestrian activity) is a difficult task. There are many variables at play. While we have presented some ideas here that we think are both interesting and useful, more work would need to be done to dig even deeper into this data, and to make more accurate models.\n",
    "\n",
    "Hopefully we have provided you with a good head start!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

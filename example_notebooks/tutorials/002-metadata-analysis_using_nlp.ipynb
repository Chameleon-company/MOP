{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://raw.githubusercontent.com/D2I-Melbourne/MOP/master/images/mop-black-100px.png)\n",
    "\n",
    "# MELBOURNE CITY OPEN DATA PLAYGROUND\n",
    "---\n",
    "## Open Data Metadata Topic Visualisation using NLP techniques\n",
    "---\n",
    "| Date | Author/Contributor | Change |\n",
    "| :- | :- | :- |\n",
    "| 30-Sep-2021 | Oscar Wu | T2 2021 Final Version |\n",
    "| 9-Dec-2021 | Steven Tuften | Format Notebook for new GitHub Repo |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTRIBUTIONS\n",
    "\n",
    "### Package/Library Imports\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Ensure you first setup Conda environment using conda configuration instructions in this Repository!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6W1OOykftSZ",
    "outputId": "4f878d2d-7100-4fff-a841-38ed30600566"
   },
   "outputs": [],
   "source": [
    "!pip install pyLDAvis==3.2.2\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install gensim\n",
    "!pip install importlib_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTOHtH3r8VE-",
    "outputId": "feefe983-2e03-4a07-9494-7400519eb822"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Open Data Specific libraries\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt# NLTK Stop words\n",
    "import seaborn as sns \n",
    "\n",
    "# NLP Libraries\n",
    "import spacy\n",
    "import spacy.cli\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Melbourne Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "apptoken = os.environ.get(\"SODAPY_APPTOKEN\") # Anonymous app token\n",
    "domain = \"data.melbourne.vic.gov.au\"\n",
    "client = Socrata(domain, apptoken) # Open Dataset connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QqPXhi28xpI"
   },
   "source": [
    "## Step 1 : Retrieve Datasets\n",
    "\n",
    "Retrieve the following information on each dataset:\n",
    "  - dataset name \n",
    "  - id \n",
    "  - metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "elyHKgNhLR-R",
    "outputId": "bc9caf45-d082-4ecf-e709-4f6553a2705a"
   },
   "outputs": [],
   "source": [
    "content = []\n",
    "for a in client.datasets():\n",
    "\n",
    "  content.append([a['resource']['name'],\n",
    "\n",
    "  a['resource']['columns_name'],\n",
    "  a['resource']['description'],\n",
    "  a['classification']['categories'],\n",
    "  a['classification']['domain_category'],\n",
    "  a['classification']['tags'],\n",
    "  a['classification']['domain_tags']])\n",
    "\n",
    "\n",
    "\n",
    "# remove '[]' and join string\n",
    "content_lst_inital = []\n",
    "for each_element in content:\n",
    "  each_element_list = []\n",
    "  for element in each_element:\n",
    "    if type(element) == list:\n",
    "      element = ' '.join(element)\n",
    "    else:\n",
    "      pass\n",
    "    each_element_list.append(element)\n",
    "  cont= (' ').join(each_element_list)\n",
    "  content_lst_inital.append(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OpFbUYYzLWkP"
   },
   "outputs": [],
   "source": [
    "content = []\n",
    "for a in client.datasets():\n",
    "\n",
    "  content.append([a['resource']['name'],\n",
    "  a['resource']['columns_name'],\n",
    "  a['resource']['description'],\n",
    "  a['classification']['categories'],\n",
    "  a['classification']['domain_category'],\n",
    "  a['classification']['tags'],\n",
    "  a['classification']['domain_tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BbP5R9OHLHC-"
   },
   "outputs": [],
   "source": [
    "total_list = []\n",
    "for each_sent in content:\n",
    "  test = []\n",
    "  for i in each_sent:\n",
    "    if type(i) == list:\n",
    "      i = \" \".join(i)\n",
    "      test.append(i)\n",
    "    else:\n",
    "      test.append(i)\n",
    "  line_list = \" \".join(test)\n",
    "  total_list.append(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "04jv0Vj9Z5yQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'counts per  .    '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[\\(\\)]',\"\", '(counts per ) (. ) ( ) (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LC0Gnbx-XLFJ"
   },
   "outputs": [],
   "source": [
    "# remove emails\n",
    "data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in total_list]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "# html tags\n",
    "data = [re.sub(r\"<.*?> \", \"\", sent) for sent in data]\n",
    "\n",
    "#\n",
    "data = [re.sub(r'[^a-zA-Z]+', \" \", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uAr5yGnoXvdZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pedestrian', 'counting', 'system', 'monthly', 'counts', 'per', 'hour', 'id', 'sensor', 'name', 'sensor', 'id', 'mdate', 'hourly', 'counts', 'month', 'year', 'date', 'time', 'time', 'day', 'this', 'dataset', 'contains', 'hourly', 'pedestrian', 'counts', 'since', 'from', 'pedestrian', 'sensor', 'devices', 'located', 'across', 'the', 'city', 'the', 'data', 'is', 'updated', 'on', 'monthly', 'basis', 'and', 'can', 'be', 'used', 'to', 'determine', 'variations', 'in', 'pedestrian', 'activity', 'throughout', 'the', 'day', 'dataset', 'which', 'details', 'the', 'location', 'status', 'and', 'directional', 'readings', 'of', 'sensors', 'any', 'changes', 'to', 'sensor', 'locations', 'are', 'important', 'to', 'consider', 'when', 'analysing', 'and', 'interpreting', 'pedestrian', 'counts', 'over', 'time', 'dataset', 'helps', 'to', 'understand', 'how', 'people', 'use', 'different', 'city', 'locations', 'at', 'different', 'times', 'of', 'day', 'to', 'better', 'inform', 'decision', 'making', 'and', 'plan', 'for', 'the', 'future', 'representation', 'of', 'pedestrian', 'volume', 'which', 'compares', 'each', 'location', 'on', 'any', 'given', 'day', 'and', 'time', 'can', 'be', 'found', 'in', 'our', 'finance', 'transport', 'accessibility', 'covid', 'foot', 'traffic', 'pedestrian', 'count', 'pedestrian', 'sensors', 'safemobility', 'sensors', 'traffic', 'flow']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "H4urvb04enl6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pedestrian', 'counting', 'system', 'monthly', 'counts', 'per', 'hour', 'id', 'sensor', 'name', 'sensor', 'id', 'mdate', 'hourly', 'counts', 'month', 'year', 'date', 'time', 'time', 'day', 'this', 'dataset', 'contains', 'hourly', 'pedestrian', 'counts', 'since', 'from', 'pedestrian', 'sensor', 'devices', 'located', 'across', 'the', 'city', 'the', 'data', 'is', 'updated', 'on', 'monthly', 'basis', 'and', 'can', 'be', 'used', 'to', 'determine', 'variations', 'in', 'pedestrian', 'activity', 'throughout', 'the', 'day', 'dataset', 'which', 'details', 'the', 'location', 'status', 'and', 'directional', 'readings', 'of', 'sensors', 'any', 'changes', 'to', 'sensor', 'locations', 'are', 'important', 'to', 'consider', 'when', 'analysing', 'and', 'interpreting', 'pedestrian', 'counts', 'over', 'time', 'dataset', 'helps', 'to', 'understand', 'how', 'people', 'use', 'different', 'city', 'locations', 'at', 'different', 'times', 'of', 'day', 'to', 'better', 'inform', 'decision', 'making', 'and', 'plan', 'for', 'the', 'future', 'representation', 'of', 'pedestrian', 'volume', 'which', 'compares', 'each', 'location', 'on', 'any', 'given', 'day', 'and', 'time', 'can', 'be', 'found', 'in', 'our', 'finance', 'transport', 'accessibility', 'covid', 'foot', 'traffic', 'pedestrian', 'count', 'pedestrian', 'sensors', 'safemobility', 'sensors', 'traffic', 'flow']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kI3z-Yj0ewqL"
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2IybIg4ybAOr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pedestrian', 'counting', 'system', 'monthly', 'count', 'hour', 'd', 'sensor', 'name', 'sensor', 'd', 'mdate', 'hourly', 'count', 'month', 'year', 'date', 'time', 'time', 'day', 'dataset', 'contain', 'hourly', 'pedestrian', 'count', 'pedestrian', 'sensor', 'device', 'locate', 'city', 'datum', 'update', 'monthly', 'basis', 'use', 'determine', 'variation', 'pedestrian', 'activity', 'day', 'dataset', 'detail', 'location', 'status', 'directional', 'reading', 'sensor', 'change', 'sensor', 'location', 'important', 'consider', 'analyse', 'interpret', 'pedestrian', 'count', 'time', 'dataset', 'help', 'understand', 'people', 'different', 'city', 'location', 'different', 'time', 'day', 'well', 'inform', 'decision', 'make', 'plan', 'future', 'representation', 'pedestrian', 'volume', 'compare', 'location', 'give', 'day', 'time', 'find', 'finance', 'transport', 'accessibility', 'covid', 'foot', 'traffic', 'pedestrian', 'count', 'pedestrian', 'safemobility', 'sensor', 'traffic', 'flow']]\n"
     ]
    }
   ],
   "source": [
    "# Download English language model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "avQIk4xjVVyx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pedestrian',\n",
       "  'counting',\n",
       "  'system',\n",
       "  'monthly',\n",
       "  'count',\n",
       "  'hour',\n",
       "  'd',\n",
       "  'sensor',\n",
       "  'name',\n",
       "  'sensor',\n",
       "  'd',\n",
       "  'mdate',\n",
       "  'hourly',\n",
       "  'count',\n",
       "  'month',\n",
       "  'year',\n",
       "  'date',\n",
       "  'time',\n",
       "  'time',\n",
       "  'day',\n",
       "  'dataset',\n",
       "  'contain',\n",
       "  'hourly',\n",
       "  'pedestrian',\n",
       "  'count',\n",
       "  'pedestrian',\n",
       "  'sensor',\n",
       "  'device',\n",
       "  'locate',\n",
       "  'city',\n",
       "  'datum',\n",
       "  'update',\n",
       "  'monthly',\n",
       "  'basis',\n",
       "  'use',\n",
       "  'determine',\n",
       "  'variation',\n",
       "  'pedestrian',\n",
       "  'activity',\n",
       "  'day',\n",
       "  'dataset',\n",
       "  'detail',\n",
       "  'location',\n",
       "  'status',\n",
       "  'directional',\n",
       "  'reading',\n",
       "  'sensor',\n",
       "  'change',\n",
       "  'sensor',\n",
       "  'location',\n",
       "  'important',\n",
       "  'consider',\n",
       "  'analyse',\n",
       "  'interpret',\n",
       "  'pedestrian',\n",
       "  'count',\n",
       "  'time',\n",
       "  'dataset',\n",
       "  'help',\n",
       "  'understand',\n",
       "  'people',\n",
       "  'different',\n",
       "  'city',\n",
       "  'location',\n",
       "  'different',\n",
       "  'time',\n",
       "  'day',\n",
       "  'well',\n",
       "  'inform',\n",
       "  'decision',\n",
       "  'make',\n",
       "  'plan',\n",
       "  'future',\n",
       "  'representation',\n",
       "  'pedestrian',\n",
       "  'volume',\n",
       "  'compare',\n",
       "  'location',\n",
       "  'give',\n",
       "  'day',\n",
       "  'time',\n",
       "  'find',\n",
       "  'finance',\n",
       "  'transport',\n",
       "  'accessibility',\n",
       "  'covid',\n",
       "  'foot',\n",
       "  'traffic',\n",
       "  'pedestrian',\n",
       "  'count',\n",
       "  'pedestrian',\n",
       "  'safemobility',\n",
       "  'sensor',\n",
       "  'traffic',\n",
       "  'flow'],\n",
       " ['tree_canopie',\n",
       "  'urban_forest',\n",
       "  'tree_canopy',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'mapped_use',\n",
       "  'aerial_photo',\n",
       "  'lidar',\n",
       "  'canopy',\n",
       "  'polygons_represent',\n",
       "  'actual',\n",
       "  'tree_canopy',\n",
       "  'extent',\n",
       "  'private',\n",
       "  'public',\n",
       "  'property',\n",
       "  'city',\n",
       "  'datum',\n",
       "  'consider',\n",
       "  'accurate',\n",
       "  'change',\n",
       "  'expect',\n",
       "  'occur',\n",
       "  'time',\n",
       "  'environment',\n",
       "  'environment',\n",
       "  'forest',\n",
       "  'tree',\n",
       "  'urbanforest'],\n",
       " ['tree',\n",
       "  'specie',\n",
       "  'dimension',\n",
       "  'urban_forest',\n",
       "  'northing',\n",
       "  'date',\n",
       "  'plant',\n",
       "  'diameter',\n",
       "  'breast',\n",
       "  'height',\n",
       "  'genus',\n",
       "  'longitude',\n",
       "  'uploaddate',\n",
       "  'scientific',\n",
       "  'name',\n",
       "  'com',\n",
       "  'i',\n",
       "  'd',\n",
       "  'latitude',\n",
       "  'useful',\n",
       "  'life',\n",
       "  'expectency',\n",
       "  'value',\n",
       "  'common',\n",
       "  'name',\n",
       "  'year',\n",
       "  'plant',\n",
       "  'family',\n",
       "  'easte',\n",
       "  'address',\n",
       "  'precinct',\n",
       "  'useful',\n",
       "  'life',\n",
       "  'expectency',\n",
       "  'state',\n",
       "  'zip',\n",
       "  'age',\n",
       "  'description',\n",
       "  'locate',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'maintain',\n",
       "  'tree',\n",
       "  'dataset',\n",
       "  'detail',\n",
       "  'location',\n",
       "  'specie',\n",
       "  'lifespan',\n",
       "  'melbourne',\n",
       "  'precinct',\n",
       "  'explore',\n",
       "  'melbourne',\n",
       "  'tree',\n",
       "  'datum',\n",
       "  'learn',\n",
       "  'life',\n",
       "  'expectancy',\n",
       "  'diversity',\n",
       "  'tree',\n",
       "  'city',\n",
       "  'check',\n",
       "  'interactive',\n",
       "  'tree',\n",
       "  'map',\n",
       "  'http',\n",
       "  'com',\n",
       "  'au',\n",
       "  'download',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'urban_for',\n",
       "  'strategy',\n",
       "  'summary',\n",
       "  'precinct',\n",
       "  'consultation',\n",
       "  'attachment',\n",
       "  'section',\n",
       "  'select',\n",
       "  'button',\n",
       "  'environment',\n",
       "  'demographic',\n",
       "  'environment',\n",
       "  'accessibility',\n",
       "  'changemaker',\n",
       "  'park',\n",
       "  'researcher',\n",
       "  'street',\n",
       "  'tree',\n",
       "  'tree',\n",
       "  'urbanfor',\n",
       "  'urbanfor'],\n",
       " ['street',\n",
       "  'parking',\n",
       "  'marker',\n",
       "  'i',\n",
       "  'd',\n",
       "  'status',\n",
       "  'location',\n",
       "  'location',\n",
       "  'address',\n",
       "  'location',\n",
       "  'city',\n",
       "  'location',\n",
       "  'state',\n",
       "  'location',\n",
       "  'zip',\n",
       "  'municipal',\n",
       "  'boundary',\n",
       "  'contain',\n",
       "  'information',\n",
       "  'ground',\n",
       "  'car',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'city',\n",
       "  'status',\n",
       "  'indicate',\n",
       "  'car',\n",
       "  'present',\n",
       "  'present',\n",
       "  'spatial',\n",
       "  'coordinate',\n",
       "  'sensor',\n",
       "  'street',\n",
       "  'marker',\n",
       "  'i',\n",
       "  'd',\n",
       "  'access',\n",
       "  'recent',\n",
       "  'datum',\n",
       "  'information',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'find',\n",
       "  'dataset',\n",
       "  'make',\n",
       "  'live',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'release',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'dataset',\n",
       "  'update',\n",
       "  'network',\n",
       "  'relay',\n",
       "  'disrupt',\n",
       "  'cause',\n",
       "  'delay',\n",
       "  'datum',\n",
       "  'update',\n",
       "  'user',\n",
       "  'datum',\n",
       "  'encourage',\n",
       "  'monitor',\n",
       "  'dataset',\n",
       "  'last',\n",
       "  'update',\n",
       "  'timestamp',\n",
       "  'check',\n",
       "  'delay',\n",
       "  'datum',\n",
       "  'update',\n",
       "  'take',\n",
       "  'ensure',\n",
       "  'data',\n",
       "  'website',\n",
       "  'accurate',\n",
       "  'current',\n",
       "  'available',\n",
       "  'note',\n",
       "  'error',\n",
       "  'omission',\n",
       "  'occasion',\n",
       "  'datum',\n",
       "  'available',\n",
       "  'website',\n",
       "  'unavailable',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'employee',\n",
       "  'accept',\n",
       "  'responsibility',\n",
       "  'loss',\n",
       "  'damage',\n",
       "  'claim',\n",
       "  'expense',\n",
       "  'cost',\n",
       "  'liability',\n",
       "  'whatsoever',\n",
       "  'include',\n",
       "  'contract',\n",
       "  'tort',\n",
       "  'include',\n",
       "  'negligence',\n",
       "  'pursuant',\n",
       "  'statue',\n",
       "  'otherwise',\n",
       "  'arise',\n",
       "  'respect',\n",
       "  'connection',\n",
       "  'access',\n",
       "  'use',\n",
       "  'datum',\n",
       "  'website',\n",
       "  'unavailability',\n",
       "  'data',\n",
       "  'website',\n",
       "  'transportation',\n",
       "  'economy',\n",
       "  'transport',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'real',\n",
       "  'time',\n",
       "  'vacancy',\n",
       "  'transport',\n",
       "  'travel',\n",
       "  'disability',\n",
       "  'accessibility',\n",
       "  'sensor',\n",
       "  'safemobility'],\n",
       " ['pay_stay',\n",
       "  'zone',\n",
       "  'link',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'zone',\n",
       "  'street',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'street',\n",
       "  'dataset',\n",
       "  'contain',\n",
       "  'pay_stay',\n",
       "  'zone',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'link',\n",
       "  'zone',\n",
       "  'go',\n",
       "  'multiple',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'also',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'multiple',\n",
       "  'pay_stay',\n",
       "  'zone',\n",
       "  'datum',\n",
       "  'link',\n",
       "  'sign',\n",
       "  'plate',\n",
       "  'zone',\n",
       "  'dataset',\n",
       "  'au',\n",
       "  'transport',\n",
       "  'movement',\n",
       "  'sign',\n",
       "  'plate',\n",
       "  'locate',\n",
       "  'zone',\n",
       "  'wwkr',\n",
       "  'also',\n",
       "  'pay_stay',\n",
       "  'parking',\n",
       "  'restriction',\n",
       "  'dataset',\n",
       "  'au',\n",
       "  'transport',\n",
       "  'movement',\n",
       "  'pay_stay',\n",
       "  'parking',\n",
       "  'restriction',\n",
       "  'ambt',\n",
       "  'qg',\n",
       "  'dataset',\n",
       "  'join',\n",
       "  'zone',\n",
       "  'create',\n",
       "  'spatial',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'join',\n",
       "  'road',\n",
       "  'street',\n",
       "  'segment',\n",
       "  'datum',\n",
       "  'property',\n",
       "  'planning',\n",
       "  'road',\n",
       "  'corridor',\n",
       "  'mdh',\n",
       "  'paystay',\n",
       "  'com',\n",
       "  'transportation',\n",
       "  'transport',\n",
       "  'parking',\n",
       "  'street',\n",
       "  'travel'],\n",
       " ['median',\n",
       "  'type',\n",
       "  'sale',\n",
       "  'year',\n",
       "  'type',\n",
       "  'sale',\n",
       "  'year',\n",
       "  'transaction',\n",
       "  'count',\n",
       "  'price',\n",
       "  'median',\n",
       "  'price',\n",
       "  'dwelling',\n",
       "  'townhouse',\n",
       "  'apartment',\n",
       "  'year',\n",
       "  'sale',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'property',\n",
       "  'growth',\n",
       "  'city',\n",
       "  'house_price',\n",
       "  'house',\n",
       "  'sale',\n",
       "  'property',\n",
       "  'residence',\n",
       "  'stat'],\n",
       " ['street',\n",
       "  'parking',\n",
       "  'bay',\n",
       "  'dataset',\n",
       "  'contain',\n",
       "  'spatial',\n",
       "  'polygons_represent',\n",
       "  'parking',\n",
       "  'bay',\n",
       "  'also',\n",
       "  'link',\n",
       "  'parking',\n",
       "  'meter',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'information',\n",
       "  'datum',\n",
       "  'join',\n",
       "  'dataset',\n",
       "  'make',\n",
       "  'live',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'release',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'bay',\n",
       "  'park',\n",
       "  'way',\n",
       "  'dataset',\n",
       "  'join',\n",
       "  'follow',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'join',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'bay',\n",
       "  'marker',\n",
       "  'attribute',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'sensor',\n",
       "  'join',\n",
       "  'street',\n",
       "  'car',\n",
       "  'park',\n",
       "  'attribute',\n",
       "  'street',\n",
       "  'parking',\n",
       "  'bay',\n",
       "  'park',\n",
       "  'currently',\n",
       "  'join',\n",
       "  'see',\n",
       "  'city',\n",
       "  'melbourne',\n",
       "  'disclaimer',\n",
       "  'regard',\n",
       "  'datum',\n",
       "  'datum',\n",
       "  'au',\n",
       "  'story',\n",
       "  'transportation',\n",
       "  'transport',\n",
       "  'accessibility',\n",
       "  'area',\n",
       "  'bay',\n",
       "  'car',\n",
       "  'parking',\n",
       "  'transport',\n",
       "  'truck'],\n",
       " ['cafe',\n",
       "  'restaurant',\n",
       "  'seat',\n",
       "  'capacity',\n",
       "  'location',\n",
       "  'coordinate',\n",
       "  'number',\n",
       "  'seat',\n",
       "  'industry',\n",
       "  'anzsic',\n",
       "  'description',\n",
       "  'industry',\n",
       "  'anzsic',\n",
       "  'code',\n",
       "  'business',\n",
       "  'address',\n",
       "  'clue',\n",
       "  'small',\n",
       "  'area',\n",
       "  'building',\n",
       "  'address',\n",
       "  'base',\n",
       "  'property',\n",
       "  'i',\n",
       "  'd',\n",
       "  'property',\n",
       "  'i',\n",
       "  'd',\n",
       "  'block',\n",
       "  'i',\n",
       "  'd',\n",
       "  'census',\n",
       "  'year',\n",
       "  'seating',\n",
       "  'type',\n",
       "  'datum',\n",
       "  'collect',\n",
       "  'part',\n",
       "  'land',\n",
       "  'employment',\n",
       "  'clue',\n",
       "  'datum',\n",
       "  'cover',\n",
       "  'period',\n",
       "  'show',\n",
       "  'business',\n",
       "  'establishment',\n",
       "  'business',\n",
       "  'address',\n",
       "  'industry',\n",
       "  'anzsic',\n",
       "  'classification',\n",
       "  'number',\n",
       "  'dining',\n",
       "  'seat',\n",
       "  'classify',\n",
       "  'indoor',\n",
       "  'outdoor',\n",
       "  'location',\n",
       "  'clue',\n",
       "  'block',\n",
       "  'small',\n",
       "  'area',\n",
       "  'designation',\n",
       "  'information',\n",
       "  'clue',\n",
       "  'see',\n",
       "  'melbourne',\n",
       "  'au',\n",
       "  'clue',\n",
       "  'economy',\n",
       "  'politic',\n",
       "  'demographic',\n",
       "  'business',\n",
       "  'beverage',\n",
       "  'business',\n",
       "  'land',\n",
       "  'employment',\n",
       "  'clue',\n",
       "  'food',\n",
       "  'industry',\n",
       "  'restaurant']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_Xn37Jyydr8I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 5), (10, 1), (11, 1), (12, 2), (13, 3), (14, 1), (15, 1), (16, 4), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 4), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 8), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 6), (49, 1), (50, 1), (51, 5), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "k0IxCWNddvqt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('accessibility', 1),\n",
       "  ('activity', 1),\n",
       "  ('analyse', 1),\n",
       "  ('basis', 1),\n",
       "  ('change', 1),\n",
       "  ('city', 2),\n",
       "  ('compare', 1),\n",
       "  ('consider', 1),\n",
       "  ('contain', 1),\n",
       "  ('count', 5),\n",
       "  ('counting', 1),\n",
       "  ('covid', 1),\n",
       "  ('d', 2),\n",
       "  ('dataset', 3),\n",
       "  ('date', 1),\n",
       "  ('datum', 1),\n",
       "  ('day', 4),\n",
       "  ('decision', 1),\n",
       "  ('detail', 1),\n",
       "  ('determine', 1),\n",
       "  ('device', 1),\n",
       "  ('different', 2),\n",
       "  ('directional', 1),\n",
       "  ('finance', 1),\n",
       "  ('find', 1),\n",
       "  ('flow', 1),\n",
       "  ('foot', 1),\n",
       "  ('future', 1),\n",
       "  ('give', 1),\n",
       "  ('help', 1),\n",
       "  ('hour', 1),\n",
       "  ('hourly', 2),\n",
       "  ('important', 1),\n",
       "  ('inform', 1),\n",
       "  ('interpret', 1),\n",
       "  ('locate', 1),\n",
       "  ('location', 4),\n",
       "  ('make', 1),\n",
       "  ('mdate', 1),\n",
       "  ('month', 1),\n",
       "  ('monthly', 2),\n",
       "  ('name', 1),\n",
       "  ('pedestrian', 8),\n",
       "  ('people', 1),\n",
       "  ('plan', 1),\n",
       "  ('reading', 1),\n",
       "  ('representation', 1),\n",
       "  ('safemobility', 1),\n",
       "  ('sensor', 6),\n",
       "  ('status', 1),\n",
       "  ('system', 1),\n",
       "  ('time', 5),\n",
       "  ('traffic', 2),\n",
       "  ('transport', 1),\n",
       "  ('understand', 1),\n",
       "  ('update', 1),\n",
       "  ('use', 1),\n",
       "  ('variation', 1),\n",
       "  ('volume', 1),\n",
       "  ('well', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_OMYFVCeOVr"
   },
   "source": [
    "## Meta Data cleaning Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7norQiFNWW1J"
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=14, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "dSSF1ofJfU4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.063*\"building\" + 0.058*\"energy\" + 0.055*\"datum\" + 0.044*\"dataset\" + '\n",
      "  '0.027*\"scope\" + 0.025*\"council\" + 0.024*\"model\" + 0.023*\"build\" + '\n",
      "  '0.023*\"attribute\" + 0.023*\"environment\"'),\n",
      " (1,\n",
      "  '0.048*\"city\" + 0.046*\"melbourne\" + 0.024*\"baseline\" + 0.024*\"single\" + '\n",
      "  '0.021*\"group\" + 0.020*\"location\" + 0.020*\"bin\" + 0.017*\"process\" + '\n",
      "  '0.017*\"specie\" + 0.014*\"online\"'),\n",
      " (2,\n",
      "  '0.059*\"parking\" + 0.040*\"datum\" + 0.031*\"time\" + 0.030*\"restriction\" + '\n",
      "  '0.026*\"sensor\" + 0.023*\"record\" + 0.023*\"street\" + 0.021*\"value\" + '\n",
      "  '0.018*\"bay\" + 0.017*\"transport\"'),\n",
      " (3,\n",
      "  '0.058*\"weight\" + 0.055*\"collect\" + 0.032*\"commingle\" + 0.032*\"waste\" + '\n",
      "  '0.031*\"record\" + 0.031*\"recycling_facility\" + 0.022*\"bale\" + 0.022*\"day\" + '\n",
      "  '0.021*\"estimate\" + 0.021*\"financial\"'),\n",
      " (4,\n",
      "  '0.068*\"city\" + 0.053*\"melbourne\" + 0.030*\"business\" + 0.027*\"environment\" + '\n",
      "  '0.024*\"property\" + 0.018*\"tree\" + 0.018*\"http\" + 0.017*\"public\" + '\n",
      "  '0.017*\"residential\" + 0.016*\"open\"'),\n",
      " (5,\n",
      "  '0.075*\"soil\" + 0.036*\"footprint\" + 0.030*\"property\" + 0.022*\"boundary\" + '\n",
      "  '0.021*\"structure\" + 0.021*\"building\" + 0.020*\"fill\" + 0.016*\"build\" + '\n",
      "  '0.015*\"derive\" + 0.013*\"order\"'),\n",
      " (6,\n",
      "  '0.140*\"location\" + 0.056*\"asset\" + 0.036*\"address\" + 0.033*\"city\" + '\n",
      "  '0.029*\"state\" + 0.028*\"zip\" + 0.027*\"banner\" + 0.023*\"description\" + '\n",
      "  '0.023*\"type\" + 0.022*\"road\"'),\n",
      " (7,\n",
      "  '0.073*\"indicator\" + 0.045*\"management\" + 0.035*\"resident\" + 0.035*\"result\" '\n",
      "  '+ 0.034*\"survey\" + 0.034*\"melbourne\" + 0.026*\"gender\" + 0.024*\"population\" '\n",
      "  '+ 0.019*\"provide\" + 0.019*\"health\"'),\n",
      " (8,\n",
      "  '0.078*\"recycling\" + 0.037*\"request\" + 0.034*\"allocate\" + 0.023*\"tree\" + '\n",
      "  '0.023*\"date\" + 0.020*\"service\" + 0.018*\"customer\" + 0.017*\"zone\" + '\n",
      "  '0.016*\"plaque\" + 0.015*\"walk\"'),\n",
      " (9,\n",
      "  '0.049*\"available\" + 0.031*\"find\" + 0.029*\"city\" + 0.028*\"melbourne\" + '\n",
      "  '0.028*\"page\" + 0.019*\"co_ordinate\" + 0.017*\"imagery\" + 0.017*\"open\" + '\n",
      "  '0.016*\"dataset\" + 0.015*\"category\"'),\n",
      " (10,\n",
      "  '0.068*\"forecast\" + 0.060*\"leg\" + 0.054*\"area\" + 0.053*\"small\" + '\n",
      "  '0.042*\"year\" + 0.020*\"dwelling\" + 0.020*\"city\" + 0.020*\"planning\" + '\n",
      "  '0.019*\"also\" + 0.019*\"growth\"'),\n",
      " (11,\n",
      "  '0.031*\"year\" + 0.026*\"city\" + 0.020*\"value\" + 0.020*\"melbourne\" + '\n",
      "  '0.019*\"plan\" + 0.019*\"event\" + 0.016*\"statement\" + 0.016*\"source\" + '\n",
      "  '0.016*\"finance\" + 0.015*\"dataset\"'),\n",
      " (12,\n",
      "  '0.045*\"clue\" + 0.038*\"service\" + 0.036*\"datum\" + 0.030*\"block\" + '\n",
      "  '0.029*\"industry\" + 0.028*\"information\" + 0.024*\"area\" + 0.021*\"anzsic\" + '\n",
      "  '0.021*\"space\" + 0.020*\"business\"'),\n",
      " (13,\n",
      "  '0.075*\"sensor\" + 0.050*\"time\" + 0.042*\"reading\" + 0.038*\"check\" + '\n",
      "  '0.037*\"bin_spot\" + 0.032*\"site\" + 0.031*\"location\" + 0.027*\"column\" + '\n",
      "  '0.025*\"dataset\" + 0.024*\"contain\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoG6LZohV8jC"
   },
   "source": [
    "# NOTE : Workbook cells beyond this point require testing and fixes!\n",
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SrWPj6SOVoAq"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3af4665cd12f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Visualize the topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'gensim'"
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-kVGnFTCXHLi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\61412\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\61412\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "gv6tUle-Nug4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_char = 'trees'\n",
    "\n",
    "lemmatizer.lemmatize(each_char, get_wordnet_pos(each_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEJ0Be34Kmmz"
   },
   "outputs": [],
   "source": [
    "nlp_content = []\n",
    "for each_sentence in words:\n",
    "  list_1 = []\n",
    "  for each_char in nltk.word_tokenize(each_sentence):\n",
    "    tk=lemmatizer.lemmatize(each_char, get_wordnet_pos(each_char))\n",
    "    list_1.append(tk)\n",
    "  nlp_content.append(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSah2iTEhU1t"
   },
   "outputs": [],
   "source": [
    "nlp_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpDsNWcTRpCo"
   },
   "outputs": [],
   "source": [
    "## check sparsicity \n",
    "\n",
    "# make it to sentence \n",
    "# nlp_content_string= sentence \n",
    "# nlp_content = nlp_content_string.split(' ')\n",
    "nlp_content_string = [' '.join(i) for i in nlp_content]\n",
    "\n",
    "\n",
    "#remove built-in english stopwords, convert all words to lowercase, \n",
    "#and a word can contain numbers and alphabets of at least length 3 in order to be qualified as a word.\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLVpBTAHRuma"
   },
   "outputs": [],
   "source": [
    "data_vectorized = vectorizer.fit_transform(nlp_content_string)\n",
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvLEUY_ut693"
   },
   "outputs": [],
   "source": [
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knpl8cA6SA8j"
   },
   "outputs": [],
   "source": [
    "search_params = {'n_components': list(range(1,223)), 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-06sx2Ehiuyg"
   },
   "outputs": [],
   "source": [
    "test_model_14 =LatentDirichletAllocation(batch_size=6, n_components = 14,\n",
    "                        random_state=0,\n",
    "                        learning_decay=0.7,\n",
    "                        max_iter= 10,\n",
    "                        max_doc_update_iter=100,\n",
    "                        learning_method='batch')\n",
    "test_model_14.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "726ItfqAILKE"
   },
   "outputs": [],
   "source": [
    "test_model_6 =LatentDirichletAllocation(batch_size=6, n_components = 6,\n",
    "                        random_state=0,\n",
    "                        learning_decay=0.7,\n",
    "                        max_iter= 10,\n",
    "                        max_doc_update_iter=100,\n",
    "                        learning_method='batch')\n",
    "test_model_6.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jtvO2-K9jTV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data is data_vectorized\n",
    "def return_topic_visual_pro_topic(model, data=data_vectorized):\n",
    "\n",
    "  # Create Document - Topic Matrix\n",
    "  lda_output = model.transform(data)\n",
    "  # column names\n",
    "  topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "  # index names\n",
    "  docnames = [\"dataset\" + str(i) for i in range(data.shape[0])]\n",
    "\n",
    "  # Make the pandas dataframe\n",
    "  df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "  # Get dominant topic for each document\n",
    "  dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "  df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "  # Styling\n",
    "  def color_green(val):\n",
    "      color = 'green' if val > .1 else 'black'\n",
    "      return 'color: {col}'.format(col=color)\n",
    "\n",
    "  def make_bold(val):\n",
    "      weight = 700 if val > .1 else 400\n",
    "      return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "\n",
    "  # Apply Style\n",
    "  df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "  \n",
    "\n",
    "  return  df_document_topics, dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXUCA8buHfb3"
   },
   "outputs": [],
   "source": [
    "# replace model to see difference \n",
    "df_document_topics, dominant_topic  = return_topic_visual_pro_topic(test_model_14, data_vectorized)\n",
    "df_document_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wSM3QDIuwE"
   },
   "outputs": [],
   "source": [
    "#topic_n = df_document_topic['dominant_topic']\n",
    "#df.loc[:,'topic_n'] = topic_n.values\n",
    "df['content'] = nlp_content_string\n",
    "df['topic_n'] = dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YM2aaxvjCzaj"
   },
   "outputs": [],
   "source": [
    "df[df.loc[:,'topic_n'] == 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diL3GPBZD30Q"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(test_model_14, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
